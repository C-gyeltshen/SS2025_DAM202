{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nxYqvolu31R",
        "outputId": "a9be14a1-a724-4f23-f3d5-62b7fc31191f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "import math"
      ],
      "metadata": {
        "id": "di5bb1bAvhej"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOKENIZER"
      ],
      "metadata": {
        "id": "zbWpk4nQwf6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer:\n",
        "    def __init__(self, vocab_size=5000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1, \"<BOS>\": 2, \"<EOS>\": 3}\n",
        "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
        "        self.next_idx = 4\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        counter = Counter()\n",
        "        for text in texts:\n",
        "            counter.update(text.lower().split())\n",
        "\n",
        "        for word, _ in counter.most_common(self.vocab_size - 4):\n",
        "            self.word2idx[word] = self.next_idx\n",
        "            self.idx2word[self.next_idx] = word\n",
        "            self.next_idx += 1\n",
        "\n",
        "    def encode(self, text):\n",
        "        tokens = text.lower().split()\n",
        "        return [self.word2idx.get(t, 1) for t in tokens]\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return ' '.join(self.idx2word.get(t, \"<UNK>\") for t in tokens if t not in [0, 2, 3])\n"
      ],
      "metadata": {
        "id": "xHSLiL04wab6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET"
      ],
      "metadata": {
        "id": "vETFiPMRwi8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WikiDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, seq_length=128, max_samples=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_length = seq_length\n",
        "        self.sequences = []\n",
        "\n",
        "        # Handle both list of texts and file path\n",
        "        if isinstance(texts, str):\n",
        "            with open(texts, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                texts = f.readlines()\n",
        "\n",
        "        if max_samples:\n",
        "            texts = texts[:max_samples]\n",
        "\n",
        "        # Tokenize and create sequences\n",
        "        for text in texts:\n",
        "            tokens = tokenizer.encode(text.strip())\n",
        "            if len(tokens) > 5:\n",
        "                # Create overlapping sequences\n",
        "                for i in range(0, len(tokens) - 1, max(1, self.seq_length // 4)):\n",
        "                    seq = tokens[i:i + self.seq_length]\n",
        "                    if len(seq) >= 10:\n",
        "                        self.sequences.append(seq)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.sequences[idx]\n",
        "\n",
        "        # Pad sequence\n",
        "        if len(seq) < self.seq_length:\n",
        "            seq = seq + [0] * (self.seq_length - len(seq))\n",
        "        else:\n",
        "            seq = seq[:self.seq_length]\n",
        "\n",
        "        # Input and target (shifted by 1)\n",
        "        x = torch.tensor(seq[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(seq[1:], dtype=torch.long)\n",
        "\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "oe151SDzwg81"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len=2048):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]"
      ],
      "metadata": {
        "id": "fbuX1FQy0IGQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRANSFORMER BLOCKS"
      ],
      "metadata": {
        "id": "nOso4-KYwpnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.scale = math.sqrt(self.head_dim)\n",
        "\n",
        "        self.q_proj = nn.Linear(d_model, d_model)\n",
        "        self.k_proj = nn.Linear(d_model, d_model)\n",
        "        self.v_proj = nn.Linear(d_model, d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
        "\n",
        "        # Causal mask (for decoder)\n",
        "        if mask is None:\n",
        "            mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n",
        "            scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.attn_dropout(attn)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        out = out.view(batch_size, seq_len, self.d_model)\n",
        "        out = self.out_proj(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.gelu(self.linear1(x))))\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Self-attention with pre-normalization (better convergence)\n",
        "        attn_out = self.attn(self.norm1(x))\n",
        "        x = x + self.dropout(attn_out)\n",
        "\n",
        "        # Feed-forward with pre-normalization\n",
        "        ff_out = self.ff(self.norm2(x))\n",
        "        x = x + self.dropout(ff_out)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "6w3yy4YRwnK4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DECODER TRANSFORMER"
      ],
      "metadata": {
        "id": "gV3KFiZXw5eR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=384, n_heads=6, n_layers=4,\n",
        "                 d_ff=1536, max_seq_len=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderBlock(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights for better convergence\"\"\"\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embedding\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Decoder layers\n",
        "        for layer in self.decoder_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Output\n",
        "        x = self.norm(x)\n",
        "        logits = self.output_proj(x)\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "dJmTsjhgw0dp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING"
      ],
      "metadata": {
        "id": "rKoKvdPIw_3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradualWarmupScheduler(optim.lr_scheduler.LambdaLR):\n",
        "    def __init__(self, optimizer, warmup_steps, total_steps):\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.total_steps = total_steps\n",
        "        super().__init__(optimizer, self.lr_lambda)\n",
        "\n",
        "    def lr_lambda(self, step):\n",
        "        if step < self.warmup_steps:\n",
        "            return float(step) / float(max(1, self.warmup_steps))\n",
        "        return max(0.0, float(self.total_steps - step) / float(max(1, self.total_steps - self.warmup_steps)))\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device, epochs=15, base_lr=1e-3):\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=base_lr, weight_decay=0.01, betas=(0.9, 0.98))\n",
        "\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    warmup_steps = len(train_loader)  # 1 epoch warmup\n",
        "    scheduler = GradualWarmupScheduler(optimizer, warmup_steps, total_steps)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 3\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits.view(-1, logits.shape[-1]), y.view(-1))\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            if (batch_idx + 1) % 50 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_loader:\n",
        "                x, y = x.to(device), y.to(device)\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits.view(-1, logits.shape[-1]), y.view(-1))\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\\n\")\n",
        "\n",
        "        # Early stopping\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"/content/best_model.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                model.load_state_dict(torch.load(\"/content/best_model.pt\"))\n",
        "                break\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "gGs7IkN2w9YL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GENERATION"
      ],
      "metadata": {
        "id": "JAmj7CvVxGGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, tokenizer, prompt, max_len=100, device='cpu', temperature=0.9, top_k=50):\n",
        "    model.eval()\n",
        "    tokens = tokenizer.encode(prompt) + [2]  # Add BOS token\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            x = torch.tensor([tokens[-127:]], dtype=torch.long).to(device)\n",
        "            logits = model(x)\n",
        "            logits = logits[0, -1, :] / temperature\n",
        "\n",
        "            # Top-k sampling\n",
        "            top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
        "            probs = F.softmax(top_k_logits, dim=-1)\n",
        "            next_token = top_k_indices[torch.multinomial(probs, 1)].item()\n",
        "\n",
        "            if next_token == 3:  # EOS token\n",
        "                break\n",
        "\n",
        "            tokens.append(next_token)\n",
        "\n",
        "    return tokenizer.decode(tokens[1:])"
      ],
      "metadata": {
        "id": "lakBcuSMxDT2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN"
      ],
      "metadata": {
        "id": "qHP_TKCUxK8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Configuration - OPTIMIZED\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    VOCAB_SIZE = 5000\n",
        "    D_MODEL = 384  # Reduced for faster training\n",
        "    N_HEADS = 6    # Reduced\n",
        "    N_LAYERS = 4   # Reduced\n",
        "    D_FF = 1536    # Proportional to d_model\n",
        "    BATCH_SIZE = 64  # Increased for better gradient estimates\n",
        "    EPOCHS = 15\n",
        "    SEQ_LENGTH = 128\n",
        "    BASE_LR = 2e-3  # Higher initial learning rate\n",
        "\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    print(f\"Model config: d_model={D_MODEL}, n_heads={N_HEADS}, n_layers={N_LAYERS}\")\n",
        "\n",
        "    # Load data\n",
        "    data_path = \"/content/gdrive/MyDrive/practical_data/assignment4\"\n",
        "\n",
        "    # Read Wikipedia sentences\n",
        "    all_texts = []\n",
        "    print(\"Loading data...\")\n",
        "    for file in os.listdir(data_path):\n",
        "        if file.endswith('.txt'):\n",
        "            with open(os.path.join(data_path, file), 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                all_texts.extend(f.readlines()[:10000])  # More samples for better training\n",
        "\n",
        "    print(f\"Loaded {len(all_texts)} texts\")\n",
        "\n",
        "    # Build tokenizer\n",
        "    print(\"Building tokenizer...\")\n",
        "    tokenizer = SimpleTokenizer(VOCAB_SIZE)\n",
        "    tokenizer.build_vocab(all_texts)\n",
        "    print(f\"Vocabulary size: {len(tokenizer.word2idx)}\")\n",
        "\n",
        "    # Split data\n",
        "    split_idx = int(0.8 * len(all_texts))\n",
        "    train_texts = all_texts[:split_idx]\n",
        "    val_texts = all_texts[split_idx:]\n",
        "\n",
        "    # Create datasets\n",
        "    print(\"Creating datasets...\")\n",
        "    train_dataset = WikiDataset(train_texts, tokenizer, SEQ_LENGTH)\n",
        "    val_dataset = WikiDataset(val_texts, tokenizer, SEQ_LENGTH)\n",
        "\n",
        "    print(f\"Train sequences: {len(train_dataset)}\")\n",
        "    print(f\"Val sequences: {len(val_dataset)}\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=0)\n",
        "\n",
        "    # Model\n",
        "    print(\"Building model...\")\n",
        "    model = DecoderTransformer(VOCAB_SIZE, D_MODEL, N_HEADS, N_LAYERS, D_FF).to(DEVICE)\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Train\n",
        "    print(\"Training...\")\n",
        "    model = train_model(model, train_loader, val_loader, DEVICE, EPOCHS, BASE_LR)\n",
        "\n",
        "    # Generate\n",
        "    print(\"\\nGenerating text:\")\n",
        "    prompts = [\"the quick brown\", \"artificial intelligence\", \"deep learning\"]\n",
        "    for prompt in prompts:\n",
        "        generated = generate(model, tokenizer, prompt, device=DEVICE)\n",
        "        print(f\"Prompt: '{prompt}' -> '{generated}'\")\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), \"/content/final_model.pt\")\n",
        "    print(\"\\nModel saved to /content/final_model.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52GiVYBpxI87",
        "outputId": "3ef8013b-35d5-4629-92a9-165f367cc150"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Model config: d_model=384, n_heads=6, n_layers=4\n",
            "Loading data...\n",
            "Loaded 10000 texts\n",
            "Building tokenizer...\n",
            "Vocabulary size: 5000\n",
            "Creating datasets...\n",
            "Train sequences: 7113\n",
            "Val sequences: 1957\n",
            "Building model...\n",
            "Model parameters: 10,943,624\n",
            "Training...\n",
            "Epoch 1/15, Batch 50/112, Loss: 6.0709, LR: 0.000893\n",
            "Epoch 1/15, Batch 100/112, Loss: 4.7779, LR: 0.001786\n",
            "\n",
            "Epoch 1/15 - Train Loss: 5.9635, Val Loss: 5.4205\n",
            "\n",
            "Epoch 2/15, Batch 50/112, Loss: 4.2700, LR: 0.001936\n",
            "Epoch 2/15, Batch 100/112, Loss: 3.6386, LR: 0.001872\n",
            "\n",
            "Epoch 2/15 - Train Loss: 4.2559, Val Loss: 4.9565\n",
            "\n",
            "Epoch 3/15, Batch 50/112, Loss: 3.8001, LR: 0.001793\n",
            "Epoch 3/15, Batch 100/112, Loss: 4.1239, LR: 0.001730\n",
            "\n",
            "Epoch 3/15 - Train Loss: 3.7471, Val Loss: 4.8485\n",
            "\n",
            "Epoch 4/15, Batch 50/112, Loss: 3.6247, LR: 0.001651\n",
            "Epoch 4/15, Batch 100/112, Loss: 3.7202, LR: 0.001587\n",
            "\n",
            "Epoch 4/15 - Train Loss: 3.4381, Val Loss: 4.7857\n",
            "\n",
            "Epoch 5/15, Batch 50/112, Loss: 3.3562, LR: 0.001508\n",
            "Epoch 5/15, Batch 100/112, Loss: 3.4348, LR: 0.001444\n",
            "\n",
            "Epoch 5/15 - Train Loss: 3.1736, Val Loss: 4.8372\n",
            "\n",
            "Epoch 6/15, Batch 50/112, Loss: 3.0566, LR: 0.001365\n",
            "Epoch 6/15, Batch 100/112, Loss: 2.9601, LR: 0.001301\n",
            "\n",
            "Epoch 6/15 - Train Loss: 2.9367, Val Loss: 4.9094\n",
            "\n",
            "Epoch 7/15, Batch 50/112, Loss: 2.5969, LR: 0.001222\n",
            "Epoch 7/15, Batch 100/112, Loss: 2.7053, LR: 0.001158\n",
            "\n",
            "Epoch 7/15 - Train Loss: 2.7182, Val Loss: 5.0036\n",
            "\n",
            "Early stopping at epoch 7\n",
            "\n",
            "Generating text:\n",
            "Prompt: 'the quick brown' -> '<UNK> brown and <UNK> and he released on the 26, 2017. <UNK> <UNK> <UNK> the <UNK> and the very <UNK> the <UNK> <UNK> as its german <UNK> and <UNK> <UNK> <UNK> records for the <UNK> <UNK> a <UNK> the <UNK> <UNK> <UNK> <UNK> the <UNK> <UNK> as a <UNK> <UNK> but <UNK> <UNK> his <UNK> <UNK> <UNK> <UNK> <UNK> an <UNK> and <UNK> <UNK> <UNK> <UNK> <UNK> a <UNK> <UNK> <UNK> <UNK> by <UNK> as a <UNK> <UNK> by <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> records in the <UNK> <UNK> <UNK> with the <UNK> <UNK> <UNK> <UNK> label. <UNK> <UNK> <UNK> by'\n",
            "Prompt: 'artificial intelligence' -> 'intelligence have been played at the top 8 <UNK> <UNK> new <UNK> <UNK> in april 2012. the united states, and three <UNK> which was an <UNK> their first as the other countries. on <UNK> <UNK> <UNK> <UNK> <UNK> in <UNK> from <UNK> to reach the fifth <UNK> and <UNK> <UNK> the <UNK> <UNK> hotel as the <UNK> <UNK> as the <UNK> the <UNK> in the remainder an <UNK> before <UNK> for <UNK> to be created records or in <UNK> they <UNK> <UNK> <UNK> and <UNK> <UNK> <UNK> <UNK> <UNK> by the state <UNK> <UNK> as a <UNK> as <UNK> the nintendo'\n",
            "Prompt: 'deep learning' -> '<UNK> is a <UNK> video game developed by <UNK> <UNK> <UNK> as <UNK> <UNK> and published by the st. <UNK> which the nintendo <UNK> james <UNK> and <UNK> <UNK> <UNK> and provisional designation <UNK> of the chief <UNK> and dan <UNK> and the <UNK> and <UNK> <UNK> and <UNK> <UNK> and <UNK> <UNK> of <UNK> and <UNK> and <UNK> of <UNK> and <UNK> of the <UNK> and <UNK> in the basic <UNK> in <UNK> written and <UNK> is the <UNK> and <UNK> and <UNK> of her <UNK> and <UNK> and <UNK> during the <UNK> and <UNK> and published on <UNK> to'\n",
            "\n",
            "Model saved to /content/final_model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DhxZLRNaxOrg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}