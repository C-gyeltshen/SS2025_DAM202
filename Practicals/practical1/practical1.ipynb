{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c850bb3",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd39baf6",
   "metadata": {},
   "source": [
    "## Section 0: Creating Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b381367",
   "metadata": {},
   "source": [
    "### Theory Notes\n",
    "\n",
    "Before diving into preprocessing techniques, we need a sample dataset to work with. In real-world applications, text data comes from various sources like social media posts, customer reviews, documents, or web scraping results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cf9a0e",
   "metadata": {},
   "source": [
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e4f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pandas library\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5418ed08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "\n",
    "\"When life gives you lemons, make lemonade! ðŸ™‚\",\n",
    "\n",
    "\"She bought 2 lemons for $1 at Maven Market.\",\n",
    "\n",
    "\"A dozen lemons will make a gallon of lemonade. [AllRecipes]\",\n",
    "\n",
    "\"lemon, lemon, lemons, lemon, lemon, lemons\",\n",
    "\n",
    "\"He's running to the market to get a lemon â€” there's a great sale today.\",\n",
    "\n",
    "\"Does Maven Market carry Eureka lemons or Meyer lemons?\",\n",
    "\n",
    "\"An Arnold Palmer is half lemonade, half iced tea. [Wikipedia]\",\n",
    "\n",
    "\"iced tea is my favorite\"\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a58b2dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When life gives you lemons, make lemonade! ðŸ™‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She bought 2 lemons for $1 at Maven Market.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A dozen lemons will make a gallon of lemonade....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemon, lemon, lemons, lemon, lemon, lemons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He's running to the market to get a lemon â€” th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Does Maven Market carry Eureka lemons or Meyer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>An Arnold Palmer is half lemonade, half iced t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>iced tea is my favorite</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence\n",
       "0       When life gives you lemons, make lemonade! ðŸ™‚\n",
       "1        She bought 2 lemons for $1 at Maven Market.\n",
       "2  A dozen lemons will make a gallon of lemonade....\n",
       "3         lemon, lemon, lemons, lemon, lemon, lemons\n",
       "4  He's running to the market to get a lemon â€” th...\n",
       "5  Does Maven Market carry Eureka lemons or Meyer...\n",
       "6  An Arnold Palmer is half lemonade, half iced t...\n",
       "7                            iced tea is my favorite"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Convert list to DataFrame\n",
    "\n",
    "data_df = pd.DataFrame(data, columns=['sentence'])\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "532d363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set display options to show full content\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fdfd1a",
   "metadata": {},
   "source": [
    "## Section 1: Preprocessing\n",
    "\n",
    "### 1.1 Normalization\n",
    "\n",
    "**Theory Notes**\n",
    "\n",
    "Text normalization is the process of converting text to a standard, consistent format. The most common normalization technique is converting all text to lowercase, which ensures that words like \"Apple\" and \"apple\" are treated as the same token.\n",
    "\n",
    "### Code Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92eee323",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a copy for spaCy processing\n",
    "\n",
    "spacy_df = data_df.copy()\n",
    "\n",
    "# Convert text to lowercase\n",
    "\n",
    "spacy_df['clean_sentence'] = spacy_df['sentence'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d980396d",
   "metadata": {},
   "source": [
    "### 1.2 Text Cleaning\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaf82be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be99e8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove specific citations\n",
    "\n",
    "spacy_df['clean_sentence'] = spacy_df['clean_sentence'].str.replace('[wikipedia]', '')\n",
    "\n",
    "# Advanced cleaning with regex\n",
    "\n",
    "combined = r'https?://\\S+|www\\.\\S+|<.*?>|\\S+@\\S+\\.\\S+|@\\w+|#\\w+|[^A-Za-z0-9\\s]'\n",
    "\n",
    "spacy_df['clean_sentence'] = spacy_df['clean_sentence'].str.replace(combined, ' ', regex=True)\n",
    "\n",
    "spacy_df['clean_sentence'] = spacy_df['clean_sentence'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf126e62",
   "metadata": {},
   "source": [
    "## Section 1.2: Advanced Text Processing with spaCy\n",
    "\n",
    "### Theory Notes\n",
    "\n",
    "spaCy is a powerful industrial-strength NLP library that provides advanced tokenization, lemmatization, and linguistic analysis. It offers pre-trained language models that understand grammar, syntax, and word relationships.\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da3c73a8-5836-4681-bcea-7265ce9f2642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m103.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:03\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25h\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "039c0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and install English language model\n",
    "\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "\n",
    "# Load the pre-trained pipeline\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "\n",
    "# Process a sample sentence\n",
    "\n",
    "phrase = spacy_df.clean_sentence[0] # \"when life gives you lemons make lemonade\"\n",
    "\n",
    "doc = nlp(phrase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5350f6",
   "metadata": {},
   "source": [
    "### 1.2.1 Tokenization\n",
    "\n",
    "**Theory Notes**\n",
    "\n",
    "Tokenization splits text into individual units (tokens) such as words, punctuation marks, or numbers. Modern tokenizers handle complex cases like contractions, compound words, and special characters intelligently.\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "104c9abe-49b7-4a43-87bd-8918bbee5194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[when, life, gives, you, lemons, make, lemonade]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Extract tokens as text strings\n",
    "\n",
    "[token.text for token in doc]\n",
    "\n",
    "# Output: ['when', 'life', 'gives', 'you', 'lemons', 'make', 'lemonade']\n",
    "\n",
    "\n",
    "\n",
    "# Extract tokens as spaCy objects (with linguistic attributes)\n",
    "\n",
    "[token for token in doc]\n",
    "\n",
    "# Output: [when, life, gives, you, lemons, make, lemonade]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5294b47a",
   "metadata": {},
   "source": [
    "### 1.2.2 Lemmatization\n",
    "\n",
    "**Theory Notes**\n",
    "\n",
    "Lemmatization reduces words to their base or root form (lemma) using linguistic knowledge. Unlike stemming, which simply removes suffixes, lemmatization considers the word's part of speech and meaning to find the correct root form.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "    \"running\" â†’ \"run\"\n",
    "\n",
    "    \"better\" â†’ \"good\"\n",
    "\n",
    "    \"mice\" â†’ \"mouse\"\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4188165e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when', 'life', 'give', 'you', 'lemon', 'make', 'lemonade']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Extract lemmatized forms\n",
    "\n",
    "[token.lemma_ for token in doc]\n",
    "\n",
    "# Output: ['when', 'life', 'give', 'you', 'lemon', 'make', 'lemonade']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe5fa9",
   "metadata": {},
   "source": [
    "### 1.2.3 Stop Words Removal\n",
    "\n",
    "**Theory Notes**\n",
    "\n",
    "Stop words are common words that carry little semantic meaning and are often filtered out to focus on more meaningful content. Examples include `\"the\", \"and\", \"is\", \"in\"`, etc.\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d628bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stop words: 326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'life give lemon lemonade'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# View all English stop words in spaCy\n",
    "\n",
    "list(nlp.Defaults.stop_words)\n",
    "\n",
    "print(f\"Total stop words: {len(list(nlp.Defaults.stop_words))}\") # 326 stop words\n",
    "\n",
    "\n",
    "\n",
    "# Remove stop words\n",
    "\n",
    "[token for token in doc if  not token.is_stop]\n",
    "\n",
    "# Output: [life, gives, lemons, lemonade]\n",
    "\n",
    "\n",
    "\n",
    "# Combine lemmatization and stop word removal\n",
    "\n",
    "[token.lemma_ for token in doc if  not token.is_stop]\n",
    "\n",
    "# Output: ['life', 'give', 'lemon', 'lemonade']\n",
    "\n",
    "\n",
    "\n",
    "# Convert back to sentence format\n",
    "\n",
    "norm = [token.lemma_ for token in doc if  not token.is_stop]\n",
    "\n",
    "' '.join(norm) # Output: 'life give lemon lemonade'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34827cfb",
   "metadata": {},
   "source": [
    "## Section 2: Creating Reusable Functions\n",
    "\n",
    "**Theory Notes**\n",
    "\n",
    "Creating modular, reusable functions is essential for maintainable code and consistent preprocessing across different datasets.\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9085114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       life give lemon lemonade\n",
       "1                     buy 2 lemon 1 maven market\n",
       "2          dozen lemon gallon lemonade allrecipe\n",
       "3            lemon lemon lemon lemon lemon lemon\n",
       "4          s run market lemon s great sale today\n",
       "5    maven market carry eureka lemon meyer lemon\n",
       "6       arnold palmer half lemonade half ice tea\n",
       "7                               ice tea favorite\n",
       "Name: clean_sentence, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Function for lemmatization and stop word removal\n",
    "\n",
    "def  token_lemma_stopw(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    output = [token.lemma_ for token in doc if  not token.is_stop]\n",
    "\n",
    "    return  ' '.join(output)\n",
    "\n",
    "\n",
    "\n",
    "# Apply to entire dataset\n",
    "\n",
    "spacy_df.clean_sentence.apply(token_lemma_stopw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f656a69",
   "metadata": {},
   "source": [
    "## Section 3: Complete NLP Pipeline\n",
    "\n",
    "**Theory Notes**\n",
    "\n",
    "An NLP pipeline combines multiple preprocessing steps into a single, streamlined workflow. This approach ensures consistency and makes it easy to apply the same transformations to new data.\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb30ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def  lower_replace(series):\n",
    "\n",
    "    output = series.str.lower()\n",
    "\n",
    "    combined = r'https?://\\S+|www\\.\\S+|<.*?>|\\S+@\\S+\\.\\S+|@\\w+|#\\w+|[^A-Za-z0-9\\s]'\n",
    "\n",
    "    output = output.str.replace(combined, ' ', regex=True)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def  nlp_pipeline(series):\n",
    "\n",
    "    output = lower_replace(series)\n",
    "\n",
    "    output = output.apply(token_lemma_stopw)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "# Apply complete pipeline\n",
    "\n",
    "cleaned_text = nlp_pipeline(data_df.sentence)\n",
    "\n",
    "\n",
    "\n",
    "# Save processed data for future use\n",
    "\n",
    "pd.to_pickle(cleaned_text, 'preprocessed_text.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e16fdc",
   "metadata": {},
   "source": [
    "## Section 4: Word Representation (Vectorization)\n",
    "\n",
    "**Theory Notes**\n",
    "\n",
    "Vectorization converts preprocessed text into numerical representations that machine learning algorithms can process. Text must be transformed into vectors (arrays of numbers) because algorithms cannot directly work with text strings.\n",
    "\n",
    "### 4.1 Count Vectorization (Bag of Words)\n",
    "\n",
    "**Theory Notes**\n",
    "\n",
    "Count Vectorization creates a matrix where each row represents a document and each column represents a unique word in the corpus. Cell values indicate how many times each word appears in each document. This approach ignores word order but captures word frequency.\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c60aefb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/chimigyeltshen/Desktop/Sem5/DAM202/virtual-env/lib/python3.13/site-packages (from scikit-learn) (2.3.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.1-cp313-cp313-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.1-cp313-cp313-macosx_12_0_arm64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m429.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading scipy-1.16.1-cp313-cp313-macosx_14_0_arm64.whl (20.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m436.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4/4\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.2 scikit-learn-1.7.1 scipy-1.16.1 threadpoolctl-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d979689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>allrecipe</th>\n",
       "      <th>arnold</th>\n",
       "      <th>buy</th>\n",
       "      <th>carry</th>\n",
       "      <th>dozen</th>\n",
       "      <th>eureka</th>\n",
       "      <th>favorite</th>\n",
       "      <th>gallon</th>\n",
       "      <th>give</th>\n",
       "      <th>great</th>\n",
       "      <th>...</th>\n",
       "      <th>life</th>\n",
       "      <th>market</th>\n",
       "      <th>maven</th>\n",
       "      <th>meyer</th>\n",
       "      <th>palmer</th>\n",
       "      <th>run</th>\n",
       "      <th>sale</th>\n",
       "      <th>tea</th>\n",
       "      <th>today</th>\n",
       "      <th>wikipedia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   allrecipe  arnold  buy  carry  dozen  eureka  favorite  gallon  give  \\\n",
       "0          0       0    0      0      0       0         0       0     1   \n",
       "1          0       0    1      0      0       0         0       0     0   \n",
       "2          1       0    0      0      1       0         0       1     0   \n",
       "3          0       0    0      0      0       0         0       0     0   \n",
       "4          0       0    0      0      0       0         0       0     0   \n",
       "5          0       0    0      1      0       1         0       0     0   \n",
       "6          0       1    0      0      0       0         0       0     0   \n",
       "7          0       0    0      0      0       0         1       0     0   \n",
       "\n",
       "   great  ...  life  market  maven  meyer  palmer  run  sale  tea  today  \\\n",
       "0      0  ...     1       0      0      0       0    0     0    0      0   \n",
       "1      0  ...     0       1      1      0       0    0     0    0      0   \n",
       "2      0  ...     0       0      0      0       0    0     0    0      0   \n",
       "3      0  ...     0       0      0      0       0    0     0    0      0   \n",
       "4      1  ...     0       1      0      0       0    1     1    0      1   \n",
       "5      0  ...     0       1      1      1       0    0     0    0      0   \n",
       "6      0  ...     0       0      0      0       1    0     0    1      0   \n",
       "7      0  ...     0       0      0      0       0    0     0    1      0   \n",
       "\n",
       "   wikipedia  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "5          0  \n",
       "6          1  \n",
       "7          0  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load preprocessed data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "series = pd.read_pickle('preprocessed_text.pkl')\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# Create Count Vectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "bow = cv.fit_transform(series)\n",
    "\n",
    "\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "\n",
    "pd.DataFrame(bow.toarray(), columns=cv.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8161be",
   "metadata": {},
   "source": [
    "### Advanced Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b41f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count Vectorizer with filtering\n",
    "\n",
    "cv1 = CountVectorizer(\n",
    "\n",
    "stop_words='english', # Remove English stop words\n",
    "\n",
    "ngram_range=(1,1), # Use only single words (unigrams)\n",
    "\n",
    "min_df=2  # Include words that appear in at least 2 documents\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "bow1 = cv1.fit_transform(series)\n",
    "\n",
    "bow1_df = pd.DataFrame(bow1.toarray(), columns=cv1.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "# Calculate term frequencies\n",
    "\n",
    "term_freq = bow1_df.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5718fa13",
   "metadata": {},
   "source": [
    "## Section 5: TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "**Theory Notes**\n",
    "\n",
    "**TF-IDF** addresses a key limitation of simple count vectorization by considering both term frequency (how often a word appears in a document) and inverse document frequency (how rare the word is across the entire corpus).\n",
    "\n",
    "- **Formula**: TF-IDF = TF \\times IDF\n",
    "- **TF (Term Frequency):** Number of times word appears in a document / Total words in the document\n",
    "- **IDF (Inverse Document Frequency):** log(Total number of documents / Number of documents containing the word)\n",
    "\n",
    "**Key Insight:** TF-IDF gives higher weights to words that are frequent in a specific document but rare across the corpus, making them more distinctive and informative.\n",
    "\n",
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16ccc41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# Basic TF-IDF vectorization\n",
    "\n",
    "tv = TfidfVectorizer()\n",
    "\n",
    "tvidf = tv.fit_transform(series)\n",
    "\n",
    "tvidf_df = pd.DataFrame(tvidf.toarray(), columns=tv.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF with filtering\n",
    "\n",
    "tv1 = TfidfVectorizer(min_df=2) # Words must appear in at least 2 documents\n",
    "\n",
    "tvidf1 = tv1.fit_transform(series)\n",
    "\n",
    "tvidf1_df = pd.DataFrame(tvidf1.toarray(), columns=tv1.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a948604",
   "metadata": {},
   "source": [
    "#### N-gram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bb46fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lemon                 1.583310\n",
       "lemon lemon           0.857624\n",
       "market                0.767950\n",
       "lemonade              0.743321\n",
       "ice tea               0.625522\n",
       "ice                   0.625522\n",
       "tea                   0.625522\n",
       "maven market          0.621858\n",
       "maven                 0.621858\n",
       "half                  0.505881\n",
       "tea favorite          0.493436\n",
       "favorite              0.493436\n",
       "buy lemon             0.439482\n",
       "buy                   0.439482\n",
       "lemon maven           0.439482\n",
       "life give             0.416207\n",
       "life                  0.416207\n",
       "give                  0.416207\n",
       "give lemon            0.416207\n",
       "lemon lemonade        0.416207\n",
       "lemonade allrecipe    0.358685\n",
       "lemon gallon          0.358685\n",
       "allrecipe             0.358685\n",
       "dozen                 0.358685\n",
       "gallon                0.358685\n",
       "dozen lemon           0.358685\n",
       "gallon lemonade       0.358685\n",
       "run                   0.319884\n",
       "sale today            0.319884\n",
       "market lemon          0.319884\n",
       "sale                  0.319884\n",
       "run market            0.319884\n",
       "great                 0.319884\n",
       "great sale            0.319884\n",
       "today                 0.319884\n",
       "lemon great           0.319884\n",
       "eureka lemon          0.302522\n",
       "lemon meyer           0.302522\n",
       "market carry          0.302522\n",
       "carry eureka          0.302522\n",
       "meyer                 0.302522\n",
       "meyer lemon           0.302522\n",
       "carry                 0.302522\n",
       "eureka                0.302522\n",
       "tea wikipedia         0.252941\n",
       "arnold palmer         0.252941\n",
       "half lemonade         0.252941\n",
       "palmer half           0.252941\n",
       "palmer                0.252941\n",
       "half ice              0.252941\n",
       "lemonade half         0.252941\n",
       "arnold                0.252941\n",
       "wikipedia             0.252941\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Bigram TF-IDF (pairs of consecutive words)\n",
    "\n",
    "tv2 = TfidfVectorizer(ngram_range=(1,2)) # Include both unigrams and bigrams\n",
    "\n",
    "tvidf2 = tv2.fit_transform(series)\n",
    "\n",
    "tvidf2_df = pd.DataFrame(tvidf2.toarray(), columns=tv2.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "# Analyze feature importance\n",
    "\n",
    "tvidf2_df.sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca446679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual-env (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
