{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b67d98",
   "metadata": {},
   "source": [
    "# Practical 2\n",
    "\n",
    "## Section 1: Data Preparation\n",
    "\n",
    "**Import Pandas library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaedd324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b64557",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'https://drive.google.com/drive/folders/1zyrm2nH9kFNAsQ3Gh3RvmW9uZydTt3VP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m ROOT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://drive.google.com/drive/folders/1zyrm2nH9kFNAsQ3Gh3RvmW9uZydTt3VP\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Your Working Directory\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mROOT\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://drive.google.com/drive/folders/1zyrm2nH9kFNAsQ3Gh3RvmW9uZydTt3VP'"
     ]
    }
   ],
   "source": [
    "ROOT = \"https://drive.google.com/drive/folders/1zyrm2nH9kFNAsQ3Gh3RvmW9uZydTt3VP\" # Your Working Directory\n",
    "import os\n",
    "os.chdir(ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b54b1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens the text.txt file in read mode\n",
    "\n",
    "with open('text.txt', 'r', encoding='utf-8') as f: # Remember your data set path should be specified if not in same working directory\n",
    "    texts = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c14fe32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When life gives you lemons, make lemonade! ðŸ™‚\\n',\n",
       " 'She bought 2 lemons for $1 at Maven Market.\\n',\n",
       " 'A dozen lemons will make a gallon of lemonade. [AllRecipes]\\n',\n",
       " 'lemon, lemon, lemons, lemon, lemon, lemons\\n',\n",
       " \"He's running to the market to get a lemon â€” there's a great sale today.\\n\",\n",
       " 'Does Maven Market carry Eureka lemons or Meyer lemons?\\n',\n",
       " 'An Arnold Palmer is half lemonade, half iced tea. [Wikipedia]\\n',\n",
       " 'iced tea is my favorite\\n']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df7ae387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the line breaker\n",
    "\n",
    "with open(\"text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [line.strip() for line in f if line.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a210dc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When life gives you lemons, make lemonade! ðŸ™‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She bought 2 lemons for $1 at Maven Market.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A dozen lemons will make a gallon of lemonade....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lemon, lemon, lemons, lemon, lemon, lemons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He's running to the market to get a lemon â€” th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Does Maven Market carry Eureka lemons or Meyer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>An Arnold Palmer is half lemonade, half iced t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>iced tea is my favorite</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence\n",
       "0       When life gives you lemons, make lemonade! ðŸ™‚\n",
       "1        She bought 2 lemons for $1 at Maven Market.\n",
       "2  A dozen lemons will make a gallon of lemonade....\n",
       "3         lemon, lemon, lemons, lemon, lemon, lemons\n",
       "4  He's running to the market to get a lemon â€” th...\n",
       "5  Does Maven Market carry Eureka lemons or Meyer...\n",
       "6  An Arnold Palmer is half lemonade, half iced t...\n",
       "7                            iced tea is my favorite"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert list to DataFrame\n",
    "\n",
    "data_df = pd.DataFrame(data, columns=['sentence'])\n",
    "data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57b7c1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When life gives you lemons, make lemonade! ðŸ™‚',\n",
       " 'She bought 2 lemons for $1 at Maven Market.',\n",
       " 'A dozen lemons will make a gallon of lemonade. [AllRecipes]',\n",
       " 'lemon, lemon, lemons, lemon, lemon, lemons',\n",
       " \"He's running to the market to get a lemon â€” there's a great sale today.\",\n",
       " 'Does Maven Market carry Eureka lemons or Meyer lemons?',\n",
       " 'An Arnold Palmer is half lemonade, half iced tea. [Wikipedia]',\n",
       " 'iced tea is my favorite']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff3e8e6",
   "metadata": {},
   "source": [
    "`Data Quality Assessment` by \n",
    "- Counting total documents and total words.\n",
    "\n",
    "- Collecting unique words and their frequencies.\n",
    "\n",
    "- Measuring sentence lengths and average sentence length.\n",
    "\n",
    "- Calculating vocabulary size (unique words count).\n",
    "\n",
    "- Finding the top 20 most frequent words.\n",
    "\n",
    "- Computing quality scores like vocabulary diversity, average word frequency, and rare word ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c93b145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 8\n",
      "Vocabulary size: 55\n",
      "Unique Words: {'lemonade!', '[allrecipes]', 'lemonade,', 'get', 'the', 'maven', 'at', 'will', 'market', '$1', 'for', 'tea.', 'life', 'arnold', 'a', '2', 'gallon', 'lemon', 'favorite', \"there's\", 'make', 'lemonade.', 'you', '[wikipedia]', 'when', 'running', 'she', 'meyer', 'carry', 'or', 'bought', 'of', 'ðŸ™‚', 'iced', 'sale', 'to', 'is', 'great', \"he's\", 'my', 'market.', 'lemons?', 'an', 'lemons', 'lemons,', 'tea', 'eureka', 'gives', 'does', 'â€”', 'palmer', 'lemon,', 'today.', 'dozen', 'half'}\n",
      "Average sentence length: 9.0\n",
      "Vocabulary diversity: 0.7639\n"
     ]
    }
   ],
   "source": [
    "def assess_data_quality(data):\n",
    "    \"\"\"Analyze text data quality for Word2Vec training\"\"\"\n",
    "\n",
    "    stats = {\n",
    "        'total_documents': len(texts),\n",
    "        'total_words': 0,\n",
    "        'unique_words': set(),\n",
    "        'sentence_lengths': [],\n",
    "        'word_frequencies': {}\n",
    "    }\n",
    "\n",
    "    for text in texts:\n",
    "        words = text.lower().split()\n",
    "        stats['total_words'] += len(words)\n",
    "        stats['sentence_lengths'].append(len(words))\n",
    "        stats['unique_words'].update(words)\n",
    "\n",
    "        for word in words:\n",
    "            stats['word_frequencies'][word] = stats['word_frequencies'].get(word, 0) + 1\n",
    "\n",
    "    stats['vocabulary_size'] = len(stats['unique_words'])\n",
    "    stats['avg_sentence_length'] = sum(stats['sentence_lengths']) / len(stats['sentence_lengths'])\n",
    "\n",
    "    # Find most common words\n",
    "    sorted_words = sorted(stats['word_frequencies'].items(), key=lambda x: x[1], reverse=True)\n",
    "    stats['top_words'] = sorted_words[:20]\n",
    "\n",
    "    # Quality indicators\n",
    "    stats['quality_score'] = {\n",
    "        'vocabulary_diversity': stats['vocabulary_size'] / stats['total_words'],\n",
    "        'avg_word_frequency': stats['total_words'] / stats['vocabulary_size'],\n",
    "        'rare_words_ratio': sum(1 for count in stats['word_frequencies'].values() if count == 1) / stats['vocabulary_size']\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Example usage\n",
    "quality_report = assess_data_quality(texts)\n",
    "print(f\"Total documents: {quality_report['total_documents']:,}\")\n",
    "print(f\"Vocabulary size: {quality_report['vocabulary_size']:,}\")\n",
    "print(f\"Unique Words: {quality_report['unique_words']}\")\n",
    "print(f\"Average sentence length: {quality_report['avg_sentence_length']:.1f}\")\n",
    "print(f\"Vocabulary diversity: {quality_report['quality_score']['vocabulary_diversity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d5a0c3",
   "metadata": {},
   "source": [
    "## Section 2: Text Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "082a761c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/chimigyeltshen/Desktop/Sem5/DAM202/practical/Practicals/p-venv/lib/python3.13/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/chimigyeltshen/Desktop/Sem5/DAM202/practical/Practicals/p-venv/lib/python3.13/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/chimigyeltshen/Desktop/Sem5/DAM202/practical/Practicals/p-venv/lib/python3.13/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/chimigyeltshen/Desktop/Sem5/DAM202/practical/Practicals/p-venv/lib/python3.13/site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in /Users/chimigyeltshen/Desktop/Sem5/DAM202/practical/Practicals/p-venv/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba97f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Packages\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acb690cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/chimigyeltshen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chimigyeltshen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/chimigyeltshen/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/chimigyeltshen/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/chimigyeltshen/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8c7ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedTextPreprocessor:\n",
    "    \"\"\"Comprehensive text preprocessing for Word2Vec training\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 lowercase=True,\n",
    "                 remove_punctuation=True,\n",
    "                 remove_numbers=False,\n",
    "                 remove_stopwords=False,\n",
    "                 min_word_length=2,\n",
    "                 max_word_length=50,\n",
    "                 lemmatize=False,\n",
    "                 remove_urls=True,\n",
    "                 remove_emails=True,\n",
    "                 keep_sentences=True):\n",
    "\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.min_word_length = min_word_length\n",
    "        self.max_word_length = max_word_length\n",
    "        self.lemmatize = lemmatize\n",
    "        self.remove_urls = remove_urls\n",
    "        self.remove_emails = remove_emails\n",
    "        self.keep_sentences = keep_sentences\n",
    "\n",
    "        if remove_stopwords:\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        if lemmatize:\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean individual text string\"\"\"\n",
    "\n",
    "        # Remove URLs\n",
    "        if self.remove_urls:\n",
    "            text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "        # Remove email addresses\n",
    "        if self.remove_emails:\n",
    "            text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        #Combined\n",
    "         #(r'https?://\\S+|www\\.\\S+|<.*?>|\\S+@\\S+\\.\\S+|@\\w+|#\\w+|[^A-Za-z0-9\\s])\n",
    "\n",
    "        return text\n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"Tokenize text into sentences or words\"\"\"\n",
    "\n",
    "        if self.keep_sentences:\n",
    "            # Tokenize into sentences first\n",
    "            sentences = sent_tokenize(text)\n",
    "            processed_sentences = []\n",
    "\n",
    "            for sentence in sentences:\n",
    "                words = self.process_sentence(sentence)\n",
    "                if len(words) >= 3:  # Keep sentences with at least 3 words\n",
    "                    processed_sentences.append(words)\n",
    "\n",
    "            return processed_sentences\n",
    "        else:\n",
    "            # Return single list of words\n",
    "            return self.process_sentence(text)\n",
    "\n",
    "    def process_sentence(self, sentence):\n",
    "        \"\"\"Process individual sentence\"\"\"\n",
    "\n",
    "        # Lowercase\n",
    "        if self.lowercase:\n",
    "            sentence = sentence.lower()\n",
    "\n",
    "        # Tokenize into words\n",
    "        words = word_tokenize(sentence)\n",
    "\n",
    "        processed_words = []\n",
    "        for word in words:\n",
    "\n",
    "            # Remove punctuation\n",
    "            if self.remove_punctuation:\n",
    "                word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "            # Skip if empty after punctuation removal\n",
    "            if not word:\n",
    "                continue\n",
    "\n",
    "            # Remove numbers\n",
    "            if self.remove_numbers and word.isdigit():\n",
    "                continue\n",
    "\n",
    "            # Check word length\n",
    "            if len(word) < self.min_word_length or len(word) > self.max_word_length:\n",
    "                continue\n",
    "\n",
    "            # Remove stopwords\n",
    "            if self.remove_stopwords and word in self.stop_words:\n",
    "                continue\n",
    "\n",
    "            # Lemmatize\n",
    "            if self.lemmatize:\n",
    "                word = self.lemmatizer.lemmatize(word)\n",
    "\n",
    "            processed_words.append(word)\n",
    "\n",
    "        return processed_words\n",
    "\n",
    "    def preprocess_corpus(self, texts):\n",
    "        \"\"\"Preprocess entire corpus\"\"\"\n",
    "\n",
    "        all_sentences = []\n",
    "\n",
    "        for text in texts:\n",
    "            if not isinstance(text, str):\n",
    "                continue\n",
    "\n",
    "            # Clean text\n",
    "            cleaned_text = self.clean_text(text)\n",
    "\n",
    "            # Tokenize and process\n",
    "            processed = self.tokenize_text(cleaned_text)\n",
    "\n",
    "            if self.keep_sentences:\n",
    "                all_sentences.extend(processed)\n",
    "            else:\n",
    "                all_sentences.append(processed)\n",
    "\n",
    "        return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5928f0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8 sentences\n",
      "Sample sentence: ['when', 'life', 'gives', 'you', 'lemons', 'make', 'lemonade']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "preprocessor = AdvancedTextPreprocessor(\n",
    "    lowercase=True,\n",
    "    remove_punctuation = True,\n",
    "    remove_numbers=True,\n",
    "    remove_stopwords=False,  # Keep stopwords for Word2Vec\n",
    "    lemmatize=False,  # Usually not needed for Word2Vec\n",
    "    keep_sentences=True\n",
    ")\n",
    "\n",
    "# Processing corpus\n",
    "processed_sentences = preprocessor.preprocess_corpus(texts)\n",
    "print(f\"Processed {len(processed_sentences)} sentences\")\n",
    "print(f\"Sample sentence: {processed_sentences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da8c84dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['when', 'life', 'gives', 'you', 'lemons', 'make', 'lemonade'],\n",
       " ['she', 'bought', 'lemons', 'for', 'at', 'maven', 'market'],\n",
       " ['dozen', 'lemons', 'will', 'make', 'gallon', 'of', 'lemonade']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e615b4",
   "metadata": {},
   "source": [
    "## Training Parameters\n",
    "\n",
    "Parameters Selection Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5efdef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_parameters(corpus_size, vocab_size, domain_type, computing_resources):\n",
    "    \"\"\"\n",
    "    Recommend Word2Vec parameters based on corpus characteristics\n",
    "\n",
    "    Args:\n",
    "        corpus_size: Number of sentences/documents\n",
    "        vocab_size: Unique words in vocabulary\n",
    "        domain_type: 'general', 'technical', 'social_media', 'academic'\n",
    "        computing_resources: 'limited', 'moderate', 'high'\n",
    "    \"\"\"\n",
    "\n",
    "    recommendations = {}\n",
    "\n",
    "    # Vector size based on corpus and vocab size\n",
    "    if corpus_size < 10000:\n",
    "        recommendations['vector_size'] = 50\n",
    "    elif corpus_size < 100000:\n",
    "        recommendations['vector_size'] = 100\n",
    "    elif corpus_size < 1000000:\n",
    "        recommendations['vector_size'] = 200\n",
    "    else:\n",
    "        recommendations['vector_size'] = 300\n",
    "\n",
    "    # Window size based on domain\n",
    "    domain_windows = {\n",
    "        'general': 5,\n",
    "        'technical': 3,  # More syntactic focus\n",
    "        'social_media': 4,\n",
    "        'academic': 6    # More semantic focus\n",
    "    }\n",
    "    recommendations['window'] = domain_windows.get(domain_type, 5)\n",
    "\n",
    "    # Min count based on corpus size\n",
    "    if corpus_size < 10000:\n",
    "        recommendations['min_count'] = 1\n",
    "    elif corpus_size < 100000:\n",
    "        recommendations['min_count'] = 2\n",
    "    elif corpus_size < 1000000:\n",
    "        recommendations['min_count'] = 5\n",
    "    else:\n",
    "        recommendations['min_count'] = 10\n",
    "\n",
    "    # Algorithm selection\n",
    "    if domain_type in ['technical', 'academic']:\n",
    "        recommendations['sg'] = 1  # Skip-gram for rare technical terms\n",
    "    else:\n",
    "        recommendations['sg'] = 0  # CBOW for general text\n",
    "\n",
    "    # Epochs based on corpus size and resources\n",
    "    if computing_resources == 'limited':\n",
    "        recommendations['epochs'] = 5\n",
    "    elif corpus_size < 100000:\n",
    "        recommendations['epochs'] = 15\n",
    "    else:\n",
    "        recommendations['epochs'] = 10\n",
    "\n",
    "    # Hierarchical softmax vs negative sampling\n",
    "    if vocab_size > 100000:\n",
    "        recommendations['hs'] = 1\n",
    "        recommendations['negative'] = 0\n",
    "    else:\n",
    "        recommendations['hs'] = 0\n",
    "        recommendations['negative'] = 10\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77a10570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Size: 8\n",
      "Vocabulary Size: 41\n"
     ]
    }
   ],
   "source": [
    "corpus_size = len(processed_sentences)\n",
    "print(f\"Corpus Size: {corpus_size}\")\n",
    "\n",
    "# Calculate vocabulary size (unique words in vocabulary)\n",
    "vocab = set(word for sentence in processed_sentences for word in sentence)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary Size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93813b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended parameters: {'vector_size': 50, 'window': 5, 'min_count': 1, 'sg': 0, 'epochs': 15, 'hs': 0, 'negative': 10}\n"
     ]
    }
   ],
   "source": [
    "# For this task\n",
    "params = recommend_parameters(\n",
    "    corpus_size=corpus_size,\n",
    "    vocab_size=vocab_size,\n",
    "    domain_type='general',\n",
    "    computing_resources='moderate'\n",
    ")\n",
    "print(\"Recommended parameters:\", params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ffc1d3",
   "metadata": {},
   "source": [
    "## Step-by-Step Implementation\n",
    "\n",
    "Basic Training Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dfaf3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4286653784.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m$pip install gensim\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "350123fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CallbackAny2Vec\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import time\n",
    "import multiprocessing\n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    \"\"\"Callback to log information about training progress\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(f\"Epoch #{self.epoch} start\")\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        print(f\"Epoch #{self.epoch} end - Time elapsed: {elapsed:.2f}s\")\n",
    "        self.epoch += 1\n",
    "\n",
    "def train_word2vec_model(sentences, save_path=None, **params):\n",
    "    \"\"\"\n",
    "    Train Word2Vec model with given parameters\n",
    "\n",
    "    Args:\n",
    "        sentences: List of tokenized sentences\n",
    "        save_path: Path to save the model\n",
    "        **params: Word2Vec parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # Set default parameters\n",
    "    default_params = {\n",
    "        'vector_size': 100,\n",
    "        'window': 5,\n",
    "        'min_count': 5,\n",
    "        'workers': multiprocessing.cpu_count() - 1,\n",
    "        'sg': 0,  # CBOW\n",
    "        'epochs': 10,\n",
    "        'alpha': 0.025,\n",
    "        'min_alpha': 0.0001,\n",
    "        'hs': 0,\n",
    "        'negative': 10\n",
    "    }\n",
    "\n",
    "    # Update with provided parameters\n",
    "    default_params.update(params)\n",
    "\n",
    "    print(\"Training Word2Vec model with parameters:\")\n",
    "    for key, value in default_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Add callback for progress monitoring\n",
    "    epoch_logger = EpochLogger()\n",
    "\n",
    "    # Train the model\n",
    "    print(f\"\\nTraining on {len(sentences)} sentences...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        callbacks=[epoch_logger],\n",
    "        **default_params\n",
    "    )\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "    print(f\"Vocabulary size: {len(model.wv)} words\")\n",
    "\n",
    "    # Save model if path provided\n",
    "    if save_path:\n",
    "        model.save(save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb016d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
