{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import re\n",
        "import time\n",
        "from collections import Counter\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Optional: sentencepiece for subword tokenization (recommended)\n",
        "try:\n",
        "    import sentencepiece as spm\n",
        "    HAS_SP = True\n",
        "except Exception:\n",
        "    HAS_SP = False"
      ],
      "metadata": {
        "id": "iNMfsjc0Cw5q"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CONFIG**"
      ],
      "metadata": {
        "id": "BDVMGe8NC1gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "    # Model\n",
        "    \"d_model\": 256,\n",
        "    \"nhead\": 8,\n",
        "    \"num_encoder_layers\": 4,\n",
        "    \"num_decoder_layers\": 4,\n",
        "    \"d_ff\": 1024,\n",
        "    \"dropout\": 0.1,\n",
        "\n",
        "    # Training\n",
        "    \"batch_size\": 32,\n",
        "    \"learning_rate\": 3e-5,\n",
        "    \"num_epochs\": 10,\n",
        "    \"max_encoder_len\": 512,\n",
        "    \"max_decoder_len\": 128,\n",
        "    \"gradient_accumulation_steps\": 2,\n",
        "    \"warmup_steps\": 4000,\n",
        "\n",
        "    # Tokens & data\n",
        "    \"vocab_size\": 30000,\n",
        "    \"pad_token\": \"<PAD>\",\n",
        "    \"sos_token\": \"<SOS>\",\n",
        "    \"eos_token\": \"<EOS>\",\n",
        "    \"unk_token\": \"<UNK>\",\n",
        "\n",
        "    # Optimization\n",
        "    \"label_smoothing\": 0.1,\n",
        "    \"grad_clip\": 1.0,\n",
        "\n",
        "    # Decoding\n",
        "    \"beam_size\": 4,\n",
        "    \"length_penalty\": 0.6,       # length normalization exponent\n",
        "    \"repetition_penalty\": 1.2,   # penalize repeated tokens in beam search\n",
        "\n",
        "    # Device & AMP\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
        "}\n",
        "\n",
        "CONFIG[\"use_amp\"] = True if CONFIG[\"device\"] == \"cuda\" else False\n",
        "print(f\"Device: {CONFIG['device']}, AMP: {CONFIG['use_amp']}\")\n",
        "\n",
        "# Optional: point to a trained sentencepiece model if you have one\n",
        "SENTENCEPIECE_MODEL_PATH = None  # e.g., \"bpe.model\" (set to path to use sentencepiece)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxHs_6pFC8nn",
        "outputId": "a314e5a4-62b2-47b5-e015-abe8d87eb72e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda, AMP: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenizer**"
      ],
      "metadata": {
        "id": "MdVmnxKjDBMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer:\n",
        "    \"\"\"Fallback word-level tokenizer used when sentencepiece unavailable.\"\"\"\n",
        "    def __init__(self, vocab_size=30000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.word_freq = Counter()\n",
        "        self.pad_token = CONFIG[\"pad_token\"]\n",
        "        self.sos_token = CONFIG[\"sos_token\"]\n",
        "        self.eos_token = CONFIG[\"eos_token\"]\n",
        "        self.unk_token = CONFIG[\"unk_token\"]\n",
        "        self.special_tokens = [self.pad_token, self.sos_token, self.eos_token, self.unk_token]\n",
        "        for i, t in enumerate(self.special_tokens):\n",
        "            self.word2idx[t] = i\n",
        "            self.idx2word[i] = t\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r'[^a-z0-9\\s\\.\\,\\!\\?]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return self.clean_text(text).split()\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        print(\"Building simple word-level vocab...\")\n",
        "        for t in texts:\n",
        "            self.word_freq.update(self.tokenize(t))\n",
        "        most_common = self.word_freq.most_common(self.vocab_size - len(self.special_tokens))\n",
        "        for idx, (w, _) in enumerate(most_common, start=len(self.special_tokens)):\n",
        "            self.word2idx[w] = idx\n",
        "            self.idx2word[idx] = w\n",
        "        print(\"Vocab size:\", len(self.word2idx))\n",
        "        CONFIG[\"vocab_size\"] = len(self.word2idx)\n",
        "\n",
        "    def encode(self, text, max_length=None, add_special_tokens=True):\n",
        "        toks = self.tokenize(text)\n",
        "        if add_special_tokens:\n",
        "            toks = [self.sos_token] + toks + [self.eos_token]\n",
        "        if max_length is not None:\n",
        "            toks = toks[:max_length]\n",
        "        return [self.word2idx.get(t, self.word2idx[self.unk_token]) for t in toks]\n",
        "\n",
        "    def decode(self, ids, skip_special_tokens=True):\n",
        "        toks = [self.idx2word.get(i, self.unk_token) for i in ids]\n",
        "        if skip_special_tokens:\n",
        "            toks = [t for t in toks if t not in self.special_tokens]\n",
        "        return \" \".join(toks)\n",
        "\n",
        "    @property\n",
        "    def pad_token_id(self):\n",
        "        return self.word2idx[self.pad_token]\n",
        "\n",
        "    @property\n",
        "    def sos_token_id(self):\n",
        "        return self.word2idx[self.sos_token]\n",
        "\n",
        "    @property\n",
        "    def eos_token_id(self):\n",
        "        return self.word2idx[self.eos_token]\n",
        "\n",
        "\n",
        "class SPTokenizer:\n",
        "    \"\"\"Wrapper for sentencepiece processor (expects a trained model)\"\"\"\n",
        "    def __init__(self, model_path):\n",
        "        self.sp = spm.SentencePieceProcessor(model_file=model_path)\n",
        "        # create mapping for special behavior: use SP IDs; we will use SP's bos/eos if available\n",
        "        # Note: SentencePiece has its own id space and special ids\n",
        "        self.vocab_size = self.sp.get_piece_size()\n",
        "        CONFIG[\"vocab_size\"] = self.vocab_size\n",
        "\n",
        "    def build_vocab(self, *args, **kwargs):\n",
        "        raise RuntimeError(\"SPTokenizer assumes pre-trained sentencepiece model; no build_vocab required.\")\n",
        "\n",
        "    def encode(self, text, max_length=None, add_special_tokens=True):\n",
        "        ids = self.sp.encode(text, out_type=int)\n",
        "        if add_special_tokens:\n",
        "            # use SentencePiece's bos/eos if present, else fallback to adding no extra tokens\n",
        "            # Many SP models don't have BOS/EOS; we will add nothing in that case.\n",
        "            pass\n",
        "        if max_length is not None:\n",
        "            ids = ids[:max_length]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids, skip_special_tokens=True):\n",
        "        return self.sp.decode(ids)\n",
        "\n",
        "    @property\n",
        "    def pad_token_id(self):\n",
        "        # SentencePiece doesn't reserve pad id by default — choose 0 as pad commonly used, but ensure it's safe.\n",
        "        return 0\n",
        "\n",
        "    @property\n",
        "    def sos_token_id(self):\n",
        "        # return None if SP doesn't define\n",
        "        return None\n",
        "\n",
        "    @property\n",
        "    def eos_token_id(self):\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "82eCq3cMDKoT"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset**"
      ],
      "metadata": {
        "id": "rO54yv8sDO1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self, articles: List[str], summaries: List[str], tokenizer, max_encoder_len, max_decoder_len):\n",
        "        self.articles = articles\n",
        "        self.summaries = summaries\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_encoder_len = max_encoder_len\n",
        "        self.max_decoder_len = max_decoder_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.articles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.tokenizer.encode(self.articles[idx], max_length=self.max_encoder_len, add_special_tokens=False)\n",
        "        dec = self.tokenizer.encode(self.summaries[idx], max_length=self.max_decoder_len, add_special_tokens=True)\n",
        "        target = dec[1:] + [self.tokenizer.pad_token_id]\n",
        "        return {\n",
        "            \"encoder_input\": torch.tensor(enc, dtype=torch.long),\n",
        "            \"decoder_input\": torch.tensor(dec, dtype=torch.long),\n",
        "            \"decoder_target\": torch.tensor(target, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(batch, pad_token_id):\n",
        "    encs = [item[\"encoder_input\"] for item in batch]\n",
        "    decs = [item[\"decoder_input\"] for item in batch]\n",
        "    tars = [item[\"decoder_target\"] for item in batch]\n",
        "    encs = nn.utils.rnn.pad_sequence(encs, batch_first=True, padding_value=pad_token_id)\n",
        "    decs = nn.utils.rnn.pad_sequence(decs, batch_first=True, padding_value=pad_token_id)\n",
        "    tars = nn.utils.rnn.pad_sequence(tars, batch_first=True, padding_value=pad_token_id)\n",
        "    return encs, decs, tars\n"
      ],
      "metadata": {
        "id": "VxuL74mgDU7i"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transformer components (Pre-LN)**"
      ],
      "metadata": {
        "id": "gizlLBQBDYui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1), :].to(x.dtype)\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % nhead == 0\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "        self.d_k = d_model // nhead\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        b, seq_len, d = x.size()\n",
        "        return x.view(b, seq_len, self.nhead, self.d_k).transpose(1, 2)  # (b, nhead, seq, d_k)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        x = x.transpose(1, 2).contiguous()\n",
        "        b, seq_len, n, d_k = x.size()\n",
        "        return x.view(b, seq_len, n * d_k)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        Q = self.split_heads(self.W_q(query))\n",
        "        K = self.split_heads(self.W_k(key))\n",
        "        V = self.split_heads(self.W_v(value))\n",
        "\n",
        "        # scaled dot-product\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  # (b, nhead, q, k)\n",
        "\n",
        "        if mask is not None:\n",
        "            # mask shape should be broadcastable to scores: (b, 1, 1, k) or (b, 1, q, k)\n",
        "            mask_bool = mask.to(torch.bool)\n",
        "            scores = scores.masked_fill(~mask_bool, float(-1e4))  # AMP-safe masking\n",
        "\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        out = torch.matmul(attn, V)  # (b, nhead, q, d_k)\n",
        "        out = self.combine_heads(out)\n",
        "        return self.W_o(out)\n",
        "\n",
        "\n",
        "class PositionwiseFF(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.dropout(F.relu(self.fc1(x))))\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.attn = MultiHeadAttention(d_model, nhead, dropout)\n",
        "        self.drop1 = nn.Dropout(dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ff = PositionwiseFF(d_model, d_ff, dropout)\n",
        "        self.drop2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        x_norm = self.norm1(x)\n",
        "        attn_out = self.attn(x_norm, x_norm, x_norm, src_mask)\n",
        "        x = x + self.drop1(attn_out)\n",
        "        x_norm = self.norm2(x)\n",
        "        ff_out = self.ff(x_norm)\n",
        "        x = x + self.drop2(ff_out)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.self_attn = MultiHeadAttention(d_model, nhead, dropout)\n",
        "        self.drop1 = nn.Dropout(dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, nhead, dropout)\n",
        "        self.drop2 = nn.Dropout(dropout)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.ff = PositionwiseFF(d_model, d_ff, dropout)\n",
        "        self.drop3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, memory, src_mask=None, tgt_mask=None):\n",
        "        x_norm = self.norm1(x)\n",
        "        self_attn = self.self_attn(x_norm, x_norm, x_norm, tgt_mask)\n",
        "        x = x + self.drop1(self_attn)\n",
        "        x_norm = self.norm2(x)\n",
        "        cross_attn = self.cross_attn(x_norm, memory, memory, src_mask)\n",
        "        x = x + self.drop2(cross_attn)\n",
        "        x_norm = self.norm3(x)\n",
        "        ff_out = self.ff(x_norm)\n",
        "        x = x + self.drop3(ff_out)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6,\n",
        "                 d_ff=2048, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.encoder_embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        self.decoder_embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [EncoderLayer(d_model, nhead, d_ff, dropout) for _ in range(num_encoder_layers)]\n",
        "        )\n",
        "        self.decoder_layers = nn.ModuleList(\n",
        "            [DecoderLayer(d_model, nhead, d_ff, dropout) for _ in range(num_decoder_layers)]\n",
        "        )\n",
        "\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def create_pad_mask(self, seq, pad_token_id):\n",
        "        # seq: (batch, seq_len) -> True where token is NOT pad\n",
        "        return (seq != pad_token_id).unsqueeze(1).unsqueeze(2)  # (batch,1,1,seq_len)\n",
        "\n",
        "    def create_causal_mask(self, size, device):\n",
        "        mask = torch.triu(torch.ones(size, size, dtype=torch.bool, device=device), diagonal=1)\n",
        "        return (~mask).unsqueeze(0).unsqueeze(0)  # (1,1,size,size)\n",
        "\n",
        "    def encode(self, src, src_mask=None):\n",
        "        x = self.encoder_embedding(src) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, src_mask)\n",
        "        return x\n",
        "\n",
        "    def decode(self, tgt, memory, src_mask=None, tgt_mask=None):\n",
        "        x = self.decoder_embedding(tgt) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        for layer in self.decoder_layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return x\n",
        "\n",
        "    def forward(self, src, tgt, pad_token_id):\n",
        "        src_mask = self.create_pad_mask(src, pad_token_id)\n",
        "        tgt_pad_mask = self.create_pad_mask(tgt, pad_token_id)\n",
        "        tgt_causal_mask = self.create_causal_mask(tgt.size(1), device=tgt.device)\n",
        "        tgt_mask = tgt_pad_mask & tgt_causal_mask\n",
        "        memory = self.encode(src, src_mask)\n",
        "        dec = self.decode(tgt, memory, src_mask, tgt_mask)\n",
        "        logits = self.output_projection(dec)\n",
        "        return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "HR35U5T9DhIb"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training utilities**"
      ],
      "metadata": {
        "id": "EMEmE2t6DkeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, optimizer, criterion, scaler, device, config, pad_id):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    for batch_idx, (src, tgt_input, tgt_output) in enumerate(dataloader):\n",
        "        src = src.to(device)\n",
        "        tgt_input = tgt_input.to(device)\n",
        "        tgt_output = tgt_output.to(device)\n",
        "\n",
        "        with autocast(enabled=config[\"use_amp\"]):\n",
        "            logits = model(src, tgt_input, pad_id)  # (batch, tgt_len, vocab)\n",
        "            vocab = logits.size(-1)\n",
        "            logits_flat = logits.contiguous().view(-1, vocab)\n",
        "            tgt_flat = tgt_output.contiguous().view(-1)\n",
        "\n",
        "            # compute loss in float32 if AMP is used\n",
        "            if config[\"use_amp\"]:\n",
        "                loss = criterion(logits_flat.float(), tgt_flat)\n",
        "            else:\n",
        "                loss = criterion(logits_flat, tgt_flat)\n",
        "            loss = loss / config[\"gradient_accumulation_steps\"]\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (batch_idx + 1) % config[\"gradient_accumulation_steps\"] == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"grad_clip\"])\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        num_tokens = (tgt_flat != pad_id).sum().item()\n",
        "        total_loss += loss.item() * config[\"gradient_accumulation_steps\"] * num_tokens\n",
        "        total_tokens += num_tokens\n",
        "\n",
        "        if (batch_idx + 1) % 50 == 0:\n",
        "            print(f\"  Batch {batch_idx + 1}/{len(dataloader)}, Loss: {total_loss/total_tokens:.4f}\")\n",
        "\n",
        "    return total_loss / total_tokens\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device, config, pad_id):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    with torch.no_grad():\n",
        "        for src, tgt_input, tgt_output in dataloader:\n",
        "            src = src.to(device)\n",
        "            tgt_input = tgt_input.to(device)\n",
        "            tgt_output = tgt_output.to(device)\n",
        "\n",
        "            logits = model(src, tgt_input, pad_id)\n",
        "            vocab = logits.size(-1)\n",
        "            logits_flat = logits.contiguous().view(-1, vocab)\n",
        "            tgt_flat = tgt_output.contiguous().view(-1)\n",
        "            if config[\"use_amp\"]:\n",
        "                loss = criterion(logits_flat.float(), tgt_flat)\n",
        "            else:\n",
        "                loss = criterion(logits_flat, tgt_flat)\n",
        "\n",
        "            num_tokens = (tgt_flat != pad_id).sum().item()\n",
        "            total_loss += loss.item() * num_tokens\n",
        "            total_tokens += num_tokens\n",
        "\n",
        "    return total_loss / total_tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "DSZUKUuQDp2J"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Beam search with length norm + repetition penalty**"
      ],
      "metadata": {
        "id": "vEB55FC1DuyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_repetition_penalty(scores: torch.Tensor, seq: List[int], penalty: float):\n",
        "    # scores: (vocab,) logit scores for next step\n",
        "    # seq: already generated token ids\n",
        "    if penalty == 1.0:\n",
        "        return scores\n",
        "    seen = set(seq)\n",
        "    for tok in seen:\n",
        "        # penalize repeated tokens by lowering their logits\n",
        "        scores[tok] /= penalty\n",
        "    return scores\n",
        "\n",
        "\n",
        "def beam_search_decode(model: Transformer, tokenizer, src_tokens: List[int], device, config,\n",
        "                       max_len=80, beam_size=4):\n",
        "    model.eval()\n",
        "    pad_id = tokenizer.pad_token_id\n",
        "    sos = tokenizer.sos_token_id if getattr(tokenizer, \"sos_token_id\", None) is not None else None\n",
        "    eos = tokenizer.eos_token_id if getattr(tokenizer, \"eos_token_id\", None) is not None else None\n",
        "\n",
        "    src = torch.tensor([src_tokens], dtype=torch.long).to(device)\n",
        "    src_mask = model.create_pad_mask(src, pad_id)\n",
        "    memory = model.encode(src, src_mask)\n",
        "\n",
        "    # Beam item: (score, sequence)\n",
        "    beams = [(0.0, [sos] if sos is not None else [])]  # start from SOS or empty if none\n",
        "    completed = []\n",
        "\n",
        "    for step in range(max_len):\n",
        "        all_candidates = []\n",
        "        for score, seq in beams:\n",
        "            # if already finished with EOS, carry it over\n",
        "            if eos is not None and len(seq) > 0 and seq[-1] == eos:\n",
        "                all_candidates.append((score, seq))\n",
        "                continue\n",
        "\n",
        "            tgt = torch.tensor([seq], dtype=torch.long).to(device)\n",
        "            tgt_mask = model.create_pad_mask(tgt, pad_id) & model.create_causal_mask(tgt.size(1), device=device)\n",
        "            dec_out = model.decode(tgt, memory, src_mask, tgt_mask)\n",
        "            logits = model.output_projection(dec_out)  # (1, seq_len, vocab)\n",
        "            next_logits = logits[0, -1]  # (vocab,)\n",
        "            # convert to log-probabilities\n",
        "            log_probs = F.log_softmax(next_logits, dim=-1).detach().cpu()\n",
        "\n",
        "            # apply repetition penalty (simple)\n",
        "            if config[\"repetition_penalty\"] != 1.0:\n",
        "                log_probs = apply_repetition_penalty(log_probs.clone(), seq, config[\"repetition_penalty\"])\n",
        "\n",
        "            topk = torch.topk(log_probs, beam_size)\n",
        "            for i in range(beam_size):\n",
        "                tok = int(topk.indices[i].item())\n",
        "                tok_logprob = float(topk.values[i].item())\n",
        "                new_score = score + tok_logprob\n",
        "                new_seq = seq + [tok]\n",
        "                all_candidates.append((new_score, new_seq))\n",
        "\n",
        "        # select best beams by score (then apply length normalization when finished)\n",
        "        ordered = sorted(all_candidates, key=lambda x: x[0], reverse=True)\n",
        "        beams = ordered[:beam_size]\n",
        "\n",
        "        # optionally move finished sequences to completed list\n",
        "        new_beams = []\n",
        "        for s, seq in beams:\n",
        "            if eos is not None and seq[-1] == eos:\n",
        "                completed.append((s, seq))\n",
        "            else:\n",
        "                new_beams.append((s, seq))\n",
        "        beams = new_beams\n",
        "        if len(beams) == 0:\n",
        "            break\n",
        "\n",
        "    # combine completed and ongoing beams\n",
        "    all_candidates = completed + beams\n",
        "    if len(all_candidates) == 0:\n",
        "        return \"\"  # nothing generated\n",
        "\n",
        "    # apply length normalization score / (len^alpha)\n",
        "    alpha = config[\"length_penalty\"]\n",
        "    best_score, best_seq = max(all_candidates, key=lambda x: x[0] / ((len(x[1]) ** alpha) + 1e-9))\n",
        "\n",
        "    # remove SOS if present and cut after EOS\n",
        "    if sos is not None and len(best_seq) > 0 and best_seq[0] == sos:\n",
        "        best_seq = best_seq[1:]\n",
        "    if eos is not None and eos in best_seq:\n",
        "        best_seq = best_seq[: best_seq.index(eos)]\n",
        "\n",
        "    return tokenizer.decode(best_seq, skip_special_tokens=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "_GYFzEkfDzja"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inference wrapper**"
      ],
      "metadata": {
        "id": "Knd2_e_GD3kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(model, tokenizer, article: str, device, config, max_len=80):\n",
        "    # use beam search if beam_size>1\n",
        "    src_tokens = tokenizer.encode(article, max_length=config[\"max_encoder_len\"], add_special_tokens=False)\n",
        "    if config[\"beam_size\"] > 1:\n",
        "        return beam_search_decode(model, tokenizer, src_tokens, device, config, max_len=max_len, beam_size=config[\"beam_size\"])\n",
        "    else:\n",
        "        # fallback greedy generation\n",
        "        model.eval()\n",
        "        pad_id = tokenizer.pad_token_id\n",
        "        sos = tokenizer.sos_token_id\n",
        "        eos = tokenizer.eos_token_id\n",
        "        src = torch.tensor([src_tokens], dtype=torch.long).to(device)\n",
        "        memory = model.encode(src, model.create_pad_mask(src, pad_id))\n",
        "        generated = [sos]\n",
        "        for _ in range(max_len):\n",
        "            tgt = torch.tensor([generated], dtype=torch.long).to(device)\n",
        "            tgt_mask = model.create_pad_mask(tgt, pad_id) & model.create_causal_mask(tgt.size(1), device=device)\n",
        "            dec = model.decode(tgt, memory, model.create_pad_mask(src, pad_id), tgt_mask)\n",
        "            logits = model.output_projection(dec)\n",
        "            next_tok = int(logits[0, -1].argmax().item())\n",
        "            generated.append(next_tok)\n",
        "            if eos is not None and next_tok == eos:\n",
        "                break\n",
        "        if sos is not None and generated and generated[0] == sos:\n",
        "            generated = generated[1:]\n",
        "        if eos is not None and eos in generated:\n",
        "            generated = generated[: generated.index(eos)]\n",
        "        return tokenizer.decode(generated, skip_special_tokens=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "uhfYUEPqD9kf"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Main training orchestration**"
      ],
      "metadata": {
        "id": "DCX5x2J7EAGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"IMPROVED TRANSFORMER - TRAIN + INFERENCE\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # ---------- load dataset (auto-detect columns) ----------\n",
        "    dataset_paths = [\n",
        "        \"/content/gdrive/MyDrive/practical_data/Inshorts-Cleaned-Data.xlsx\",\n",
        "        \"/content/Inshorts-Cleaned-Data.xlsx\",\n",
        "        \"Inshorts-Cleaned-Data.xlsx\",\n",
        "        \"news_summary.csv\",\n",
        "    ]\n",
        "    df = None\n",
        "    for p in dataset_paths:\n",
        "        try:\n",
        "            if p.endswith(\".xlsx\"):\n",
        "                df = pd.read_excel(p)\n",
        "                print(\"Loaded\", p)\n",
        "                break\n",
        "            elif p.endswith(\".csv\"):\n",
        "                df = pd.read_csv(p, encoding=\"latin-1\")\n",
        "                print(\"Loaded\", p)\n",
        "                break\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if df is None:\n",
        "        print(\"Dataset not found. Update dataset_paths.\")\n",
        "        return\n",
        "\n",
        "    article_cols = [\"Short\", \"short\", \"content\", \"article\", \"text\", \"news\", \"Content\", \"Article\", \"Text\"]\n",
        "    summary_cols = [\"Headline\", \"headline\", \"summary\", \"title\", \"headlines\", \"Headline\", \"Summary\", \"Title\"]\n",
        "\n",
        "    article_col = next((c for c in article_cols if c in df.columns), None)\n",
        "    summary_col = next((c for c in summary_cols if c in df.columns), None)\n",
        "\n",
        "    if article_col is None or summary_col is None:\n",
        "        print(\"Could not auto-detect article/summary columns. Columns:\", df.columns.tolist())\n",
        "        return\n",
        "\n",
        "    df = df.dropna(subset=[article_col, summary_col])\n",
        "    df = df[df[article_col].str.len() > 50]\n",
        "    df = df[df[summary_col].str.len() > 10]\n",
        "\n",
        "    sample_size = min(50000, len(df))\n",
        "    df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
        "    articles = df[article_col].astype(str).tolist()\n",
        "    summaries = df[summary_col].astype(str).tolist()\n",
        "\n",
        "    # ---------- tokenizer: prefer sentencepiece if available & model provided ----------\n",
        "    if HAS_SP and SENTENCEPIECE_MODEL_PATH:\n",
        "        print(\"Using SentencePiece tokenizer:\", SENTENCEPIECE_MODEL_PATH)\n",
        "        tokenizer = SPTokenizer(SENTENCEPIECE_MODEL_PATH)\n",
        "    else:\n",
        "        print(\"Using SimpleTokenizer (word-level). To get better results use sentencepiece BPE.\")\n",
        "        tokenizer = SimpleTokenizer(vocab_size=CONFIG[\"vocab_size\"])\n",
        "        tokenizer.build_vocab(articles + summaries)\n",
        "\n",
        "    pad_id = tokenizer.pad_token_id\n",
        "    print(\"Pad id:\", pad_id, \"Vocab size:\", CONFIG.get(\"vocab_size\"))\n",
        "\n",
        "    # ---------- dataset & loaders ----------\n",
        "    train_size = int(0.9 * len(articles))\n",
        "    train_ds = SummarizationDataset(articles[:train_size], summaries[:train_size], tokenizer, CONFIG[\"max_encoder_len\"], CONFIG[\"max_decoder_len\"])\n",
        "    val_ds = SummarizationDataset(articles[train_size:], summaries[train_size:], tokenizer, CONFIG[\"max_encoder_len\"], CONFIG[\"max_decoder_len\"])\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=CONFIG[\"batch_size\"], shuffle=True, collate_fn=lambda b: collate_fn(b, pad_id), num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=CONFIG[\"batch_size\"], shuffle=False, collate_fn=lambda b: collate_fn(b, pad_id), num_workers=2, pin_memory=True)\n",
        "\n",
        "    # ---------- model ----------\n",
        "    model = Transformer(vocab_size=CONFIG[\"vocab_size\"], d_model=CONFIG[\"d_model\"], nhead=CONFIG[\"nhead\"],\n",
        "                        num_encoder_layers=CONFIG[\"num_encoder_layers\"], num_decoder_layers=CONFIG[\"num_decoder_layers\"],\n",
        "                        d_ff=CONFIG[\"d_ff\"], dropout=CONFIG[\"dropout\"], max_len=max(CONFIG[\"max_encoder_len\"], CONFIG[\"max_decoder_len\"])\n",
        "                        ).to(CONFIG[\"device\"])\n",
        "    print(\"Model params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "    # ---------- optimizer + loss + scaler ----------\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"], betas=(0.9, 0.98), eps=1e-8)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id, label_smoothing=CONFIG[\"label_smoothing\"])\n",
        "    scaler = GradScaler(enabled=CONFIG[\"use_amp\"])\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "    for epoch in range(CONFIG[\"num_epochs\"]):\n",
        "        start_time = time.time()\n",
        "        print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, CONFIG[\"device\"], CONFIG, pad_id)\n",
        "        val_loss = evaluate(model, val_loader, criterion, CONFIG[\"device\"], CONFIG, pad_id)\n",
        "        duration = time.time() - start_time\n",
        "        print(f\"Epoch {epoch+1} — Train loss: {train_loss:.4f} Val loss: {val_loss:.4f} Time: {duration:.1f}s\")\n",
        "\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            torch.save({\"epoch\": epoch, \"model_state_dict\": model.state_dict(), \"optimizer_state_dict\": optimizer.state_dict(), \"val_loss\": val_loss, \"config\": CONFIG}, \"best_model.pt\")\n",
        "            print(\"Saved best model.\")\n",
        "\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            print(\"\\nSample generation (validation sample 0):\")\n",
        "            sample_article = articles[train_size][:500]\n",
        "            print(\"Article:\", sample_article[:300], \"...\")\n",
        "            gen = generate_summary(model, tokenizer, sample_article, CONFIG[\"device\"], CONFIG, max_len=CONFIG[\"max_decoder_len\"])\n",
        "            print(\"Generated:\", gen)\n",
        "            print(\"Reference:\", summaries[train_size][:200])\n",
        "\n",
        "    print(\"Training complete — best val:\", best_val)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElmtrxC74apT",
        "outputId": "42c798f6-8386-4bf4-efc0-098c76b94bd2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "IMPROVED TRANSFORMER - TRAIN + INFERENCE\n",
            "================================================================================\n",
            "Loaded /content/gdrive/MyDrive/practical_data/Inshorts-Cleaned-Data.xlsx\n",
            "Using SimpleTokenizer (word-level). To get better results use sentencepiece BPE.\n",
            "Building simple word-level vocab...\n",
            "Vocab size: 30000\n",
            "Pad id: 0 Vocab size: 30000\n",
            "Model params: 30442800\n",
            "\n",
            "Epoch 1/10\n",
            "  Batch 50/1407, Loss: 9.4873\n",
            "  Batch 100/1407, Loss: 8.9518\n",
            "  Batch 150/1407, Loss: 8.6469\n",
            "  Batch 200/1407, Loss: 8.4466\n",
            "  Batch 250/1407, Loss: 8.3124\n",
            "  Batch 300/1407, Loss: 8.2164\n",
            "  Batch 350/1407, Loss: 8.1482\n",
            "  Batch 400/1407, Loss: 8.0947\n",
            "  Batch 450/1407, Loss: 8.0505\n",
            "  Batch 500/1407, Loss: 8.0110\n",
            "  Batch 550/1407, Loss: 7.9778\n",
            "  Batch 600/1407, Loss: 7.9524\n",
            "  Batch 650/1407, Loss: 7.9315\n",
            "  Batch 700/1407, Loss: 7.9105\n",
            "  Batch 750/1407, Loss: 7.8906\n",
            "  Batch 800/1407, Loss: 7.8732\n",
            "  Batch 850/1407, Loss: 7.8581\n",
            "  Batch 900/1407, Loss: 7.8441\n",
            "  Batch 950/1407, Loss: 7.8301\n",
            "  Batch 1000/1407, Loss: 7.8162\n",
            "  Batch 1050/1407, Loss: 7.8059\n",
            "  Batch 1100/1407, Loss: 7.7946\n",
            "  Batch 1150/1407, Loss: 7.7840\n",
            "  Batch 1200/1407, Loss: 7.7740\n",
            "  Batch 1250/1407, Loss: 7.7624\n",
            "  Batch 1300/1407, Loss: 7.7530\n",
            "  Batch 1350/1407, Loss: 7.7444\n",
            "  Batch 1400/1407, Loss: 7.7357\n",
            "Epoch 1 — Train loss: 7.7350 Val loss: 7.4842 Time: 98.6s\n",
            "Saved best model.\n",
            "\n",
            "Epoch 2/10\n",
            "  Batch 50/1407, Loss: 7.4108\n",
            "  Batch 100/1407, Loss: 7.4025\n",
            "  Batch 150/1407, Loss: 7.4067\n",
            "  Batch 200/1407, Loss: 7.4053\n",
            "  Batch 250/1407, Loss: 7.4011\n",
            "  Batch 300/1407, Loss: 7.3958\n",
            "  Batch 350/1407, Loss: 7.3925\n",
            "  Batch 400/1407, Loss: 7.3846\n",
            "  Batch 450/1407, Loss: 7.3833\n",
            "  Batch 500/1407, Loss: 7.3768\n",
            "  Batch 550/1407, Loss: 7.3721\n",
            "  Batch 600/1407, Loss: 7.3646\n",
            "  Batch 650/1407, Loss: 7.3583\n",
            "  Batch 700/1407, Loss: 7.3518\n",
            "  Batch 750/1407, Loss: 7.3454\n",
            "  Batch 800/1407, Loss: 7.3406\n",
            "  Batch 850/1407, Loss: 7.3338\n",
            "  Batch 900/1407, Loss: 7.3269\n",
            "  Batch 950/1407, Loss: 7.3216\n",
            "  Batch 1000/1407, Loss: 7.3165\n",
            "  Batch 1050/1407, Loss: 7.3124\n",
            "  Batch 1100/1407, Loss: 7.3064\n",
            "  Batch 1150/1407, Loss: 7.2996\n",
            "  Batch 1200/1407, Loss: 7.2937\n",
            "  Batch 1250/1407, Loss: 7.2875\n",
            "  Batch 1300/1407, Loss: 7.2804\n",
            "  Batch 1350/1407, Loss: 7.2740\n",
            "  Batch 1400/1407, Loss: 7.2677\n",
            "Epoch 2 — Train loss: 7.2665 Val loss: 7.0442 Time: 102.8s\n",
            "Saved best model.\n",
            "\n",
            "Sample generation (validation sample 0):\n",
            "Article: Delhi-based telemedicine-focused startup Imedilane has raised $40,000 in a funding round from angel investors Mahendra Patel, an ex-army officer, and businessman Nilender Chauhan. Founded by Shakti Anand, along with his wife Khushboo Anand, the healthcare startup allows patients to speak with the ri ...\n",
            "Generated: startup startup raises raises\n",
            "Reference: Telemedicine-focused startup raises $40,000\n",
            "\n",
            "Epoch 3/10\n",
            "  Batch 50/1407, Loss: 6.9953\n",
            "  Batch 100/1407, Loss: 6.9941\n",
            "  Batch 150/1407, Loss: 6.9958\n",
            "  Batch 200/1407, Loss: 6.9901\n",
            "  Batch 250/1407, Loss: 6.9859\n",
            "  Batch 300/1407, Loss: 6.9808\n",
            "  Batch 350/1407, Loss: 6.9764\n",
            "  Batch 400/1407, Loss: 6.9718\n",
            "  Batch 450/1407, Loss: 6.9641\n",
            "  Batch 500/1407, Loss: 6.9571\n",
            "  Batch 550/1407, Loss: 6.9533\n",
            "  Batch 600/1407, Loss: 6.9524\n",
            "  Batch 650/1407, Loss: 6.9512\n",
            "  Batch 700/1407, Loss: 6.9514\n",
            "  Batch 750/1407, Loss: 6.9509\n",
            "  Batch 800/1407, Loss: 6.9462\n",
            "  Batch 850/1407, Loss: 6.9426\n",
            "  Batch 900/1407, Loss: 6.9403\n",
            "  Batch 950/1407, Loss: 6.9373\n",
            "  Batch 1000/1407, Loss: 6.9339\n",
            "  Batch 1050/1407, Loss: 6.9313\n",
            "  Batch 1100/1407, Loss: 6.9285\n",
            "  Batch 1150/1407, Loss: 6.9259\n",
            "  Batch 1200/1407, Loss: 6.9244\n",
            "  Batch 1250/1407, Loss: 6.9214\n",
            "  Batch 1300/1407, Loss: 6.9191\n",
            "  Batch 1350/1407, Loss: 6.9164\n",
            "  Batch 1400/1407, Loss: 6.9136\n",
            "Epoch 3 — Train loss: 6.9135 Val loss: 6.8050 Time: 98.7s\n",
            "Saved best model.\n",
            "\n",
            "Epoch 4/10\n",
            "  Batch 50/1407, Loss: 6.6758\n",
            "  Batch 100/1407, Loss: 6.6965\n",
            "  Batch 150/1407, Loss: 6.7120\n",
            "  Batch 200/1407, Loss: 6.7097\n",
            "  Batch 250/1407, Loss: 6.7125\n",
            "  Batch 300/1407, Loss: 6.7164\n",
            "  Batch 350/1407, Loss: 6.7201\n",
            "  Batch 400/1407, Loss: 6.7188\n",
            "  Batch 450/1407, Loss: 6.7237\n",
            "  Batch 500/1407, Loss: 6.7206\n",
            "  Batch 550/1407, Loss: 6.7153\n",
            "  Batch 600/1407, Loss: 6.7150\n",
            "  Batch 650/1407, Loss: 6.7158\n",
            "  Batch 700/1407, Loss: 6.7149\n",
            "  Batch 750/1407, Loss: 6.7144\n",
            "  Batch 800/1407, Loss: 6.7132\n",
            "  Batch 850/1407, Loss: 6.7112\n",
            "  Batch 900/1407, Loss: 6.7083\n",
            "  Batch 950/1407, Loss: 6.7080\n",
            "  Batch 1000/1407, Loss: 6.7062\n",
            "  Batch 1050/1407, Loss: 6.7047\n",
            "  Batch 1100/1407, Loss: 6.7026\n",
            "  Batch 1150/1407, Loss: 6.7016\n",
            "  Batch 1200/1407, Loss: 6.7006\n",
            "  Batch 1250/1407, Loss: 6.7000\n",
            "  Batch 1300/1407, Loss: 6.6983\n",
            "  Batch 1350/1407, Loss: 6.6964\n",
            "  Batch 1400/1407, Loss: 6.6928\n",
            "Epoch 4 — Train loss: 6.6924 Val loss: 6.6304 Time: 97.8s\n",
            "Saved best model.\n",
            "\n",
            "Sample generation (validation sample 0):\n",
            "Article: Delhi-based telemedicine-focused startup Imedilane has raised $40,000 in a funding round from angel investors Mahendra Patel, an ex-army officer, and businessman Nilender Chauhan. Founded by Shakti Anand, along with his wife Khushboo Anand, the healthcare startup allows patients to speak with the ri ...\n",
            "Generated: startup startup raises startup raises\n",
            "Reference: Telemedicine-focused startup raises $40,000\n",
            "\n",
            "Epoch 5/10\n",
            "  Batch 50/1407, Loss: 6.5164\n",
            "  Batch 100/1407, Loss: 6.5101\n",
            "  Batch 150/1407, Loss: 6.5073\n",
            "  Batch 200/1407, Loss: 6.5060\n",
            "  Batch 250/1407, Loss: 6.5073\n",
            "  Batch 300/1407, Loss: 6.5153\n",
            "  Batch 350/1407, Loss: 6.5161\n",
            "  Batch 400/1407, Loss: 6.5228\n",
            "  Batch 450/1407, Loss: 6.5219\n",
            "  Batch 500/1407, Loss: 6.5176\n",
            "  Batch 550/1407, Loss: 6.5164\n",
            "  Batch 600/1407, Loss: 6.5189\n",
            "  Batch 650/1407, Loss: 6.5218\n",
            "  Batch 700/1407, Loss: 6.5199\n",
            "  Batch 750/1407, Loss: 6.5206\n",
            "  Batch 800/1407, Loss: 6.5202\n",
            "  Batch 850/1407, Loss: 6.5180\n",
            "  Batch 900/1407, Loss: 6.5194\n",
            "  Batch 950/1407, Loss: 6.5172\n",
            "  Batch 1000/1407, Loss: 6.5170\n",
            "  Batch 1050/1407, Loss: 6.5164\n",
            "  Batch 1100/1407, Loss: 6.5142\n",
            "  Batch 1150/1407, Loss: 6.5132\n",
            "  Batch 1200/1407, Loss: 6.5119\n",
            "  Batch 1250/1407, Loss: 6.5105\n",
            "  Batch 1300/1407, Loss: 6.5103\n",
            "  Batch 1350/1407, Loss: 6.5088\n",
            "  Batch 1400/1407, Loss: 6.5072\n",
            "Epoch 5 — Train loss: 6.5071 Val loss: 6.4914 Time: 94.8s\n",
            "Saved best model.\n",
            "\n",
            "Epoch 6/10\n",
            "  Batch 50/1407, Loss: 6.3526\n",
            "  Batch 100/1407, Loss: 6.3463\n",
            "  Batch 150/1407, Loss: 6.3439\n",
            "  Batch 200/1407, Loss: 6.3518\n",
            "  Batch 250/1407, Loss: 6.3483\n",
            "  Batch 300/1407, Loss: 6.3460\n",
            "  Batch 350/1407, Loss: 6.3524\n",
            "  Batch 400/1407, Loss: 6.3506\n",
            "  Batch 450/1407, Loss: 6.3466\n",
            "  Batch 500/1407, Loss: 6.3452\n",
            "  Batch 550/1407, Loss: 6.3442\n",
            "  Batch 600/1407, Loss: 6.3437\n",
            "  Batch 650/1407, Loss: 6.3437\n",
            "  Batch 700/1407, Loss: 6.3435\n",
            "  Batch 750/1407, Loss: 6.3435\n",
            "  Batch 800/1407, Loss: 6.3439\n",
            "  Batch 850/1407, Loss: 6.3449\n",
            "  Batch 900/1407, Loss: 6.3452\n",
            "  Batch 950/1407, Loss: 6.3451\n",
            "  Batch 1000/1407, Loss: 6.3452\n",
            "  Batch 1050/1407, Loss: 6.3444\n",
            "  Batch 1100/1407, Loss: 6.3436\n",
            "  Batch 1150/1407, Loss: 6.3446\n",
            "  Batch 1200/1407, Loss: 6.3455\n",
            "  Batch 1250/1407, Loss: 6.3473\n",
            "  Batch 1300/1407, Loss: 6.3469\n",
            "  Batch 1350/1407, Loss: 6.3462\n",
            "  Batch 1400/1407, Loss: 6.3459\n",
            "Epoch 6 — Train loss: 6.3451 Val loss: 6.3657 Time: 94.1s\n",
            "Saved best model.\n",
            "\n",
            "Sample generation (validation sample 0):\n",
            "Article: Delhi-based telemedicine-focused startup Imedilane has raised $40,000 in a funding round from angel investors Mahendra Patel, an ex-army officer, and businessman Nilender Chauhan. Founded by Shakti Anand, along with his wife Khushboo Anand, the healthcare startup allows patients to speak with the ri ...\n",
            "Generated: startup raises startup raises funding\n",
            "Reference: Telemedicine-focused startup raises $40,000\n",
            "\n",
            "Epoch 7/10\n",
            "  Batch 50/1407, Loss: 6.1953\n",
            "  Batch 100/1407, Loss: 6.1919\n",
            "  Batch 150/1407, Loss: 6.1844\n",
            "  Batch 200/1407, Loss: 6.1832\n",
            "  Batch 250/1407, Loss: 6.1804\n",
            "  Batch 300/1407, Loss: 6.1833\n",
            "  Batch 350/1407, Loss: 6.1855\n",
            "  Batch 400/1407, Loss: 6.1939\n",
            "  Batch 450/1407, Loss: 6.1923\n",
            "  Batch 500/1407, Loss: 6.1961\n",
            "  Batch 550/1407, Loss: 6.1966\n",
            "  Batch 600/1407, Loss: 6.1977\n",
            "  Batch 650/1407, Loss: 6.1986\n",
            "  Batch 700/1407, Loss: 6.1991\n",
            "  Batch 750/1407, Loss: 6.1995\n",
            "  Batch 800/1407, Loss: 6.2005\n",
            "  Batch 850/1407, Loss: 6.1995\n",
            "  Batch 900/1407, Loss: 6.2012\n",
            "  Batch 950/1407, Loss: 6.2001\n",
            "  Batch 1000/1407, Loss: 6.1995\n",
            "  Batch 1050/1407, Loss: 6.2013\n",
            "  Batch 1100/1407, Loss: 6.1999\n",
            "  Batch 1150/1407, Loss: 6.2012\n",
            "  Batch 1200/1407, Loss: 6.2009\n",
            "  Batch 1250/1407, Loss: 6.2015\n",
            "  Batch 1300/1407, Loss: 6.2018\n",
            "  Batch 1350/1407, Loss: 6.2014\n",
            "  Batch 1400/1407, Loss: 6.2011\n",
            "Epoch 7 — Train loss: 6.2015 Val loss: 6.2726 Time: 94.5s\n",
            "Saved best model.\n",
            "\n",
            "Epoch 8/10\n",
            "  Batch 50/1407, Loss: 6.0039\n",
            "  Batch 100/1407, Loss: 6.0345\n",
            "  Batch 150/1407, Loss: 6.0377\n",
            "  Batch 200/1407, Loss: 6.0426\n",
            "  Batch 250/1407, Loss: 6.0380\n",
            "  Batch 300/1407, Loss: 6.0447\n",
            "  Batch 350/1407, Loss: 6.0428\n",
            "  Batch 400/1407, Loss: 6.0435\n",
            "  Batch 450/1407, Loss: 6.0460\n",
            "  Batch 500/1407, Loss: 6.0487\n",
            "  Batch 550/1407, Loss: 6.0505\n",
            "  Batch 600/1407, Loss: 6.0549\n",
            "  Batch 650/1407, Loss: 6.0598\n",
            "  Batch 700/1407, Loss: 6.0636\n",
            "  Batch 750/1407, Loss: 6.0636\n",
            "  Batch 800/1407, Loss: 6.0651\n",
            "  Batch 850/1407, Loss: 6.0658\n",
            "  Batch 900/1407, Loss: 6.0656\n",
            "  Batch 950/1407, Loss: 6.0656\n",
            "  Batch 1000/1407, Loss: 6.0662\n",
            "  Batch 1050/1407, Loss: 6.0670\n",
            "  Batch 1100/1407, Loss: 6.0658\n",
            "  Batch 1150/1407, Loss: 6.0676\n",
            "  Batch 1200/1407, Loss: 6.0681\n",
            "  Batch 1250/1407, Loss: 6.0696\n",
            "  Batch 1300/1407, Loss: 6.0699\n",
            "  Batch 1350/1407, Loss: 6.0706\n",
            "  Batch 1400/1407, Loss: 6.0712\n",
            "Epoch 8 — Train loss: 6.0712 Val loss: 6.1829 Time: 96.4s\n",
            "Saved best model.\n",
            "\n",
            "Sample generation (validation sample 0):\n",
            "Article: Delhi-based telemedicine-focused startup Imedilane has raised $40,000 in a funding round from angel investors Mahendra Patel, an ex-army officer, and businessman Nilender Chauhan. Founded by Shakti Anand, along with his wife Khushboo Anand, the healthcare startup allows patients to speak with the ri ...\n",
            "Generated: startup raises startup raises funding\n",
            "Reference: Telemedicine-focused startup raises $40,000\n",
            "\n",
            "Epoch 9/10\n",
            "  Batch 50/1407, Loss: 5.9535\n",
            "  Batch 100/1407, Loss: 5.9249\n",
            "  Batch 150/1407, Loss: 5.9164\n",
            "  Batch 200/1407, Loss: 5.9121\n",
            "  Batch 250/1407, Loss: 5.9153\n",
            "  Batch 300/1407, Loss: 5.9153\n",
            "  Batch 350/1407, Loss: 5.9166\n",
            "  Batch 400/1407, Loss: 5.9182\n",
            "  Batch 450/1407, Loss: 5.9246\n",
            "  Batch 500/1407, Loss: 5.9281\n",
            "  Batch 550/1407, Loss: 5.9300\n",
            "  Batch 600/1407, Loss: 5.9337\n",
            "  Batch 650/1407, Loss: 5.9368\n",
            "  Batch 700/1407, Loss: 5.9376\n",
            "  Batch 750/1407, Loss: 5.9387\n",
            "  Batch 800/1407, Loss: 5.9385\n",
            "  Batch 850/1407, Loss: 5.9408\n",
            "  Batch 900/1407, Loss: 5.9434\n",
            "  Batch 950/1407, Loss: 5.9468\n",
            "  Batch 1000/1407, Loss: 5.9485\n",
            "  Batch 1050/1407, Loss: 5.9499\n",
            "  Batch 1100/1407, Loss: 5.9507\n",
            "  Batch 1150/1407, Loss: 5.9514\n",
            "  Batch 1200/1407, Loss: 5.9524\n",
            "  Batch 1250/1407, Loss: 5.9533\n",
            "  Batch 1300/1407, Loss: 5.9537\n",
            "  Batch 1350/1407, Loss: 5.9539\n",
            "  Batch 1400/1407, Loss: 5.9540\n",
            "Epoch 9 — Train loss: 5.9544 Val loss: 6.1244 Time: 97.7s\n",
            "Saved best model.\n",
            "\n",
            "Epoch 10/10\n",
            "  Batch 50/1407, Loss: 5.8046\n",
            "  Batch 100/1407, Loss: 5.7950\n",
            "  Batch 150/1407, Loss: 5.8099\n",
            "  Batch 200/1407, Loss: 5.8047\n",
            "  Batch 250/1407, Loss: 5.8183\n",
            "  Batch 300/1407, Loss: 5.8168\n",
            "  Batch 350/1407, Loss: 5.8231\n",
            "  Batch 400/1407, Loss: 5.8250\n",
            "  Batch 450/1407, Loss: 5.8261\n",
            "  Batch 500/1407, Loss: 5.8284\n",
            "  Batch 550/1407, Loss: 5.8282\n",
            "  Batch 600/1407, Loss: 5.8295\n",
            "  Batch 650/1407, Loss: 5.8320\n",
            "  Batch 700/1407, Loss: 5.8314\n",
            "  Batch 750/1407, Loss: 5.8332\n",
            "  Batch 800/1407, Loss: 5.8331\n",
            "  Batch 850/1407, Loss: 5.8363\n",
            "  Batch 900/1407, Loss: 5.8355\n",
            "  Batch 950/1407, Loss: 5.8379\n",
            "  Batch 1000/1407, Loss: 5.8404\n",
            "  Batch 1050/1407, Loss: 5.8402\n",
            "  Batch 1100/1407, Loss: 5.8395\n",
            "  Batch 1150/1407, Loss: 5.8439\n",
            "  Batch 1200/1407, Loss: 5.8437\n",
            "  Batch 1250/1407, Loss: 5.8454\n",
            "  Batch 1300/1407, Loss: 5.8466\n",
            "  Batch 1350/1407, Loss: 5.8464\n",
            "  Batch 1400/1407, Loss: 5.8478\n",
            "Epoch 10 — Train loss: 5.8473 Val loss: 6.0590 Time: 92.7s\n",
            "Saved best model.\n",
            "\n",
            "Sample generation (validation sample 0):\n",
            "Article: Delhi-based telemedicine-focused startup Imedilane has raised $40,000 in a funding round from angel investors Mahendra Patel, an ex-army officer, and businessman Nilender Chauhan. Founded by Shakti Anand, along with his wife Khushboo Anand, the healthcare startup allows patients to speak with the ri ...\n",
            "Generated: startup raises funding from seed funding\n",
            "Reference: Telemedicine-focused startup raises $40,000\n",
            "Training complete — best val: 6.05901186145145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ZTXvMP54c19"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}